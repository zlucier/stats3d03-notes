\rhead{\color{main}2 Multivariate distributions}
\section{Multivariate distributions}

We will often want to deal with more than one variable based on the same random experiment.

\begin{definition}[random vector]
	Consider a random experiment with sample space $\Omega$. Let $X_1,X_2:\Omega\to\mathbb R$ be random variables. We say that $(X_1,X_2)$ is a \textbf{random vector}. The support of $(X_1,X_2)$ is the set of ordered pairs $\chi=\{(x_1,x_2):X_1(\omega)=x_1,X_2(\omega)=x_2,\omega\in \Omega\}$.
\end{definition}

\subsection{Joint distributions}

Of particular interest are events of the form $(X_1\leq x_1)\cap(X_2\leq x_2)$ for $(x_1,x_2)\in\mathbb R^2$.

\begin{definition}[joint cdf]
	Suppose that $(X_1,X_2)$ is a random vector. The \textbf{joint cdf} of $(X_1,X_2)$, is the function $F_{X_1,X_2}$ defined as $$F_{X_1,X_2}(x_1,x_2)=P(X_1\leq x_1,X_2\leq x_2).$$
\end{definition}

From here we can extend the univariate case for the probability over intervals to rectangular subsets of $\mathbb R^2$.

\begin{theorem}[Rectangular probability formula]
	Suppose that the random vector $(X_1,X_2)$ has joint cdf $F_{X_1,X_2}$ and let $a_1,b_1,a_2,b_2\in\mathbb R$ be such that $a_1<b_1$ and $a_2<b_2$. Then $$P\left((X_1,X_2)\in[a_1,b_1]\times[a_2,b_2]\right)=F_{X_1,X_2}(b_1,b_2)-F_{X_1,X_2}(a_1,b_2)-F_{X_1,X_2}(b_1,a_2)+F_{X_1,X_2}(a_1,a_2).$$
\end{theorem}

Recall that, in general, random variables can be of the discrete type or of the continuous type. We extend this idea to random vectors.

A random vector $(X_1,X_2)$ is said to be \textbf{discrete} if its support $\chi$ is countable. In this case both $X_1$ and $X_2$ are discrete random variables. It thus makes sense to define pmfs for discrete random vectors.

\begin{definition}[joint pmf]
	Let $(X_1,X_2)$ be a discrete random vector. Then the \textbf{joint pmf} of $(X_1,X_2)$, is the function $p_{X_1,X_2}$ given by

	$$p_{X_1,X_2}(x_1,x_2)=P(X_1=x_1,X_2=x_2).$$
\end{definition}

As in the univariate case, this joint pmf satisfies the following properties.
\begin{enumerate}
	\item $0\leq p_{X_1,X_2}(x_1,x_2)\leq 1$ for all $(x_1,x_2)\in\chi$.
	\item $\sum_{(x_1,x_2)\in\chi}p_{X_1,X_2}(x_1,x_2)=1$.
\end{enumerate}

\begin{example}[]\label{ex:dice_jointpmf}
	Suppose two dice are rolled. Let $X$ denote the number of dots facing up on the first die and $Y$ the number of dots on the second die. Also, let $W=\max(X,Y)$. We would like to find the joint pmf of the random vector $(X,W)$ and the probability that $X=W$.

	\textit{Solution.} Let $\chi$ denote the support of $(X,W)$. It is clear that $\chi=\{1,\hdots,6\}\times\{1,\hdots,6\}$.

	The joint pmf $p_{X,W}(x,w)$ of $(X,W)$ can be summarized by the following table.

	\begin{center}
	%\rowcolors{2}{white}{main!10}
	\begin{tabular}{c | c c c c c c}
		$p_{X,W}(x,w)$ & $x=1$ & $x=2$ & $x=3$ & $x=4$ & $x=5$ & $x=6$\\\hline
		$w=1$ & $\frac{1}{36}$ & 0 & 0 & 0 & 0 & 0\\
		$w=2$ & $\frac{1}{36}$ & $\frac{2}{36}$ & 0 & 0 & 0 & 0\\
		$w=3$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{3}{36}$ & 0 & 0 & 0\\
		$w=4$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{4}{36}$ & 0 & 0\\
		$w=5$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{5}{36}$ & 0\\
		$w=6$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{6}{36}$
	\end{tabular}
	\end{center}

	It is clearly the case that $0\leq p_{X,W}(x,w)\leq 1$ and $\sum_{(x,w)\in\chi}p_{X,W}(x,w)=1$.

	We can now find the probability that $X=W$. That is, that the first die has the larger number of dots. We sum along the diagonal of the above table.
	\begin{align*}
		\sum_{\substack{(x,w)\in\chi\\x=w}}p_{X,W}(x,w)&=\frac 1{36}+\frac{2}{36}+\cdots+\frac{6}{36}\\
		&=\frac{7}{12}
	\end{align*}

\end{example}

If the joint cdf $F_{X_1,X_2}$ of a random vector $(X_1,X_2)$ is continuous, then we say that $(X_1,X_2)$ is \textbf{continuous}. Similarly to a joint pmf, we can also define a joint pdf.

\begin{definition}[joint pdf]
	Let $(X_1,X_2)$ be a continuous random vector. Then the \textbf{joint pdf} of $(X_1,X_2)$ is the function $f_{X_1,X_2}$ satisfying
	$$F_{X_1,X_2}(x_1,x_2)=\int_{-\infty}^{x_2}\int_{-\infty}^{x_1}f_{X_1,X_2}(u,v)\dd u\dd v$$
	for all $(x_1,x_2)\in\mathbb R^2$.
\end{definition}

As in the univariate case, the joint pdf satisfies the following properties.
\begin{enumerate}
	\item $f_{X_1,X_2}(x_1,x_2)\geq 0$ for all $(x_1,x_2)\in\mathbb R^2$.
	\item $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{X_1,X_2}(u,v)\dd u\dd v=1$.
\end{enumerate}

Often times, we will want to obtain the distributions of the random variables $X_1$ and $X_2$ from the joint distribution of $(X_1,X_2)$.

Given the above setup, we may obtain the \textbf{marginal cdf} of $X_1$ from the following equivalent formulations.
\begin{align*}
	F_{X_1}(x_1)&=P(X_1\leq x_1)\\
	&=P(X_1\leq x_1,-\infty<X_2<\infty)\\
	&=\lim_{x_2\to\infty}P(X_1\leq x_1,X_2\leq x_2)\\
	&=\lim_{x_2\to\infty} F_{X_1,X_2}(x_1,x_2)
\end{align*}

We may also find \textbf{marginal pmfs} and \textbf{marginal pdfs}.

If $(X_1,X_2)$ is a discrete random vector, then
\begin{align*}
	p_{X_1}(x_1)&=P(X_1=x_1)\\
	&=\sum_{x_2}p_{X_1,X_2}(x_1,x_2)
\end{align*}

If $(X_1,X_2)$ is a continuous random vector, then
\begin{align*}
	f_{X_1}(x_1)&=\int_{-\infty}^{\infty}f_{X_1,X_2}(x_1,x_2)\dd{x_2}\\
\end{align*}

\begin{example}[]\label{ex:dice_marginalpmf}
	We revisit the setup of Example \ref{ex:dice_jointpmf}. We would like to find the marginal pmf of $W$.

	\textit{Solution.} We apply the definition.
	\begin{center}
	\begin{tabular}{c | c}
		$w$ & $p_W(w)$\\ \hline
		1 & $\frac{1}{36}$\\
		2 & $\frac{3}{36}$\\
		3 & $\frac{5}{36}$\\
		4 & $\frac{7}{36}$\\
		5 & $\frac{9}{36}$\\
		6 & $\frac{11}{36}$
	\end{tabular}
	\end{center}

	We may write $$p_W(w)=\begin{cases}
		\frac{2w-1}{36} & \text{if $w=1,\hdots, 6$}\\
		0 & \text{otherwise}
	\end{cases}.$$
\end{example}


\begin{example}[]\label{ex:jointpdf_cxy}
	Consider the joint pdf of $(X,Y)$ to be
	$$f_{X,Y}(x,y)=\begin{cases}
		cxy^2 & \text{for $0\leq x\leq 2,0\leq y\leq 1$}\\
		0 & \text{otherwise}
	\end{cases}$$
	We would like to find the value of $c$ such that $f_{X,Y}$ is a valid pdf. We would then like to find the marginal pdfs.

	\textit{Solution.} For $f_{X,Y}$ to be valid, we require it to be non-negative and it must integrate to 1. The second property will be used to determine $c$.

	\begin{align*}
		\int_0^1\int_0^2 cxy^2\dd x\dd y & =1\\
		c\int_0^1y^2\int_0^2 x\dd x\dd y & =1\\
		c\left(\frac 13\right)(2)&=1\\
		\frac 23 c&=1\\
		c&=\frac 32
	\end{align*}

	Such a $c$ makes $f_{X,Y}$ a valid pdf.

	We first find the marginal pdf of $X$.

	\begin{align*}
		f_X(x)&=\int_0^1\frac 32xy^2\dd y\\
		&=\frac 32x\int_0^1y^2\dd y\\
		&=\frac x2
	\end{align*}

	So $f_X(x)=\begin{cases}
		\frac x 2 & \text{for $0\leq x\leq 2$}\\
		0 & \text{otherwise}
	\end{cases}$.

	\begin{align*}
		f_Y(y)&=\int_0^2\frac 32xy^2\dd x\\
		&=\frac 32y^2\int_0^1x\dd y\\
		&=\frac{3y^2}4
	\end{align*}

	So $f_Y(x)=\begin{cases}
		\frac{3y^2}4 & \text{for $0\leq y\leq 1$}\\
		0 & \text{otherwise}
	\end{cases}$.
\end{example}

\subsection{Expectation}

Expectations in the multivariate case are easily extended from the univariate case to random vectors.

\begin{theorem}[Law of the unconscious statistician (multivariate)]\label{thm:lotus2}
	Let $(X_1,X_2)$ be a random vector and let $Y=g(X_1,X_2)$ for some real-valued function $g$. Then $Y$ is a random variable and we have the following.
	\begin{enumerate}[label=\color{main}(\alph*)]
		\item Suppose $(X_1,X_2)$ is discrete with pmf $p_{X_1,X_2}(x_1,x_2)$. If $$\sum_{x_1}\sum_{x_2}|g(x_1,x_2)|p_{X_1,X_2}(x_1,x_2)$$ is finite, then the expectation of $Y$ exists and is given by $$E(Y)=\sum_{x_1}\sum_{x_2}g(x_1,x_2)p_{X_1,X_2}(x_1,x_2).$$
		\item Suppose $(X_1,X_2)$ is continuous with pdf $f_{X_1,X_2}(x_1,x_2)$. If $$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|g(x_1,x_2)|f_{X_1,X_2}(x_1,x_2)\dd{x_1}\dd{x_2}$$ is finite, then the expectation of $Y$ exists and is given by $$E(Y)=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x_1,x_2)f_{X_1,X_2}(x_1,x_2)\dd{x_1}\dd{x_2}.$$
	\end{enumerate}
\end{theorem}

We often abbreviate this theorem LOTUS.

We can now show that expectation is a linear operator.

\begin{theorem}[Linearity of expectation]
	Let $(X_1,X_2)$ be a random vector and let $Y_1=g_1(X_1,X_2)$ and $Y_2=g_2(X_1,X_2)$ be random variables for some real-valued functions $g_1$ and $g_2$. Suppose that both $E(Y_1)$ and $E(Y_2)$ exist. Then for any $k_1,k_2\in\mathbb R$,
	$$E(k_1Y_1+k_2Y_2)=k_1E(Y_1)+k_2E(Y_2).$$
\end{theorem}

\begin{proof}
	We prove for the discrete case.

	We show absolute convergence using the triangle inequality.
	\begin{align*}
		\sum_{x_1}\sum_{x_2}|k_1g_1(x_1,x_2)+k_2g_2(x_1,x_2)|p_{X_1,X_2}(x_1,x_2)&\leq |k_1|\sum_{x_1}\sum_{x_2}|g_1(x_1,x_2)|p_{X_1,X_2}(x_1,x_2)\\
		&\quad+|k_2|\sum_{x_1}\sum_{x_2}|g_2(x_1,x_2)|p_{X_1,X_2}(x_1,x_2)
	\end{align*}

	But $E(Y_1)$ and $E(Y_2)$ exist, so the above is finite and $E(k_1Y_1+k_2Y_2)$ exists.
	\begin{align*}
		E(k_1Y_1+k_2Y_2)&=\sum_{x_1}\sum_{x_2}(k_1g_1(x_1,x_2)+k_2g_2(x_1,x_2))p_{X_1,X_2}(x_1,x_2)\\
		&=k_1\sum_{x_1}\sum_{x_2}g_1(x_1,x_2)p_{X_1,X_2}(x_1,x_2)+k_2\sum_{x_1}\sum_{x_2}g_2(x_1,x_2)p_{X_1,X_2}(x_1,x_2)\\
		&=k_1E(Y_1)+k_2E(Y_2)
	\end{align*}
\end{proof}

\begin{example}[]
	Consider the random vector $(X,Y)$ with joint pmf $p_{X,Y}(x,y)$ defined as follows.

	\begin{center}
	\begin{tabular}{c | c c c}
		$p_{X,Y}(x,y)$ & $y=1$ & $y=2$ & $y=3$\\ \hline
		$x=0$ & $\frac{1}{18}$ & $\frac{1}{9}$ & $\frac{1}{18}$\\
		$x=1$ & $\frac{1}{9}$ & $\frac{1}{9}$ & $\frac{1}{18}$\\
		$x=2$ & $\frac{2}{9}$ & $\frac{1}{18}$ & $\frac{2}{9}$
	\end{tabular}
	\end{center}

	Find $E(XY)$.

	\textit{Solution.} Let $g(X,Y)=XY$. We use Theorem \ref{thm:lotus2} (LOTUS).
	\begin{align*}
		E(X,Y)&=\sum_{x}\sum_{y}xyp_{X,Y}(xy)\\
		&=(0)(1)\frac{1}{18}+(0)(2)\frac{1}{9}+(0)(3)\frac{1}{18}\\
		&\quad +(1)(1)\frac{1}{9}+(1)(2)\frac{1}{9}+(1)(3)\frac{1}{18}\\
		&\quad +(2)(1)\frac{2}{9}+(2)(2)\frac{1}{18}+(2)(3)\frac{9}{9}\\
		&=\frac{45}{18}
	\end{align*}
\end{example}

\begin{example}[]\label{ex:jointpdf_E(XY)}
	Recall the setup of Example \ref{ex:jointpdf_cxy}. Find $E(XY)$.

	\textit{Solution.}

	\begin{align*}
		E(XY)&=\int_0^1\int_0^2(xy)\frac 32xy^2\dd x\dd y\\
		&=\frac 32\int_0^1y^3\int_0^2x^2\dd x\dd y\\
		&=\frac 32\int_0^1y^3\frac 83\dd y\\
		&=4\int_0^1y^3\dd y\\
		&=4\cdot\frac 14\\
		&=1
	\end{align*}
\end{example}

\subsection{Joint measures}

For an event $B$ in the support of the discrete random vector $(X,Y)$ with pmf $p_{X,Y}$,
$$P((X,Y)\in B)=\sum_{(x,y)\in B}p_{X,Y}(x,y).$$

For an event $A$ in the support of the continuous random vector $(X,Y)$ with pdf $f_{X,Y}$,
$$P((X,Y)\in A)=\iint_Af_{X,Y}(x,y)\dd x\dd y.$$

\begin{example}[]
	Recall the setup of Example \ref{ex:jointpdf_cxy}. Find $P(X+Y\leq 2)$.

	\textit{Solution.} Let $A$ denote the event $X+Y\leq 2$. Then
	\begin{align*}
		P((X,Y)\in A)=\int\int_A\frac 32xy^2\dd y\dd x.
	\end{align*}


	The support of $(X,Y)$ is $\chi=\{(x,y):0\leq x\leq 2,0\leq y\leq 1\}$ and the set of points under and on the line $X+Y=2$ is $\{(x,y)\in\mathbb R^2:y\leq 2-x\}$. As such, $A$ is the intersection of these sets and, notably, $A=\{(x,y)\in\chi:y\leq 2-x\}$. We illustrate these sets in the following plot.

	\begin{center}
	\begin{tikzpicture}
	\begin{axis}[
	GridPlot1,
	xmin=0,
	xmax=2.25,
	ymin=0,
	ymax=2.25,
	xtick={0,1,2},
	ytick={0,1,2},
	xlabel={$X$},
	ylabel={$Y$},
    scale=0.75
	%axis equal
	]
	\path[name path=axis] (axis cs:0,0) -- (axis cs:2,0);
	\path[name path=y1] (0,1) -- (2,1);

	\node at (1.5,1.25) {\color{indigo}$\chi$};

	 \addplot[
        thick,
        color=indigo,
        fill opacity=0.2]
    fill between[
        of=y1 and axis,
        soft clip={domain=0:2},
    ];

    \addplot[name path=f,domain=0:2,main,thick] {2-x};

	\addplot[
        thick,
        %color=blue,
        fill=main, 
        fill opacity=0.2]
    fill between[
        of=f and axis,
        soft clip={domain=0:2},
    ];

	\end{axis}
	\end{tikzpicture}
	\end{center}
	We can write $A$ as a disjoint union $A=A_1\cup A_2$, as shown in the following plot.

	\begin{center}
	\begin{tikzpicture}
	\begin{axis}[
	GridPlot1,
	xmin=0,
	xmax=2.25,
	ymin=0,
	ymax=2.25,
	xtick={0,1,2},
	ytick={0,1,2},
	xlabel={$X$},
	ylabel={$Y$},
    scale=0.75
	%axis equal
	]
	\path[name path=axis] (axis cs:0,0) -- (axis cs:2,0);
	\path[name path=y1] (0,1) -- (2,1);

	 \addplot[
        color=pink,
        fill opacity=0.2]
    fill between[
        of=y1 and axis,
        soft clip={domain=0:1},
    ];

    \addplot[name path=f,domain=0:2,main,thick] {2-x};

	\addplot[
        color=green,
        fill opacity=0.2]
    fill between[
        of=f and axis,
        soft clip={domain=1:2},
    ];

    \node at (0.5,0.5) {\color{pink}$A_1$};
    \node at (1.35,0.35) {\color{green}$A_2$};

	\end{axis}
	\end{tikzpicture}
	\end{center}

	We see that $A_1=\{(x,y):0\leq x\leq y\leq 1\}$ and $A_2=\{(x,y):1\leq x\leq 2,0\leq y\leq 2-x\}$. So we can compute the required probability as follows.

	$$P((X,Y)\in A)=\iint_Af_{X,Y}(x,y)\dd y\dd x=\iint_{A_1}f_{X,Y}(x,y)\dd y\dd x+\iint_{A_2}f_{X,Y}(x,y)\dd y\dd x$$

	We solve each integral.

	\begin{align*}
	\iint_{A_1}f_{X,Y}(x,y)\dd y\dd x&=\int_0^1\int_0^1\frac 32xy^2\dd y\dd x\\
		&=\frac 32\int_0^1x\int_0^1y^2\dd y\dd x\\
		&=\frac 32\int_0^1x\frac 13\dd x\\
		&=\frac 12\int_0^1x\dd x\\
		&=\frac 12\cdot\frac 12\\
		&=\frac 14
	\end{align*}

	\begin{align*}
		\iint_{A_2}f_{X,Y}(x,y)\dd y\dd x&=\int_1^2\int_{0}^{2-x}\frac 32xy^2\dd y\dd x\\
		&=\frac 32\int_1^2x\int_{0}^{2-x}y^2\dd y\dd x\\
		&=\frac 32\int_1^2x\frac{(2-x)^3}{3}\dd y\dd x\\
		&=\frac 12\int_1^2x (2-x)^3\dd y\dd x\\
		&=\frac 12\int_1^2-x^4+6x^3-12x^2+8x\dd x\\
		&=\frac 12\cdot\frac 3{10}\\
		&=\frac 3{20}
	\end{align*}

	Adding the two integrals, $P(X+Y\leq 2)=\frac 14+\frac 3{20}=\frac 8{20}$.
\end{example}

\begin{definition}[covariance]
	Suppose $(X,Y)$ is a random vector and that $E(X)$ and $E(Y)$ exist. The \textbf{covariance} of $(X,Y)$, denoted $\Cov(X,Y)$, is defined by the expectation

	$$\Cov(X,Y)=E((X-E(X))(Y-E(Y))).$$
\end{definition}

We may obtain a computationally favourable formulation of covariance using the linearity of expectation.
\begin{align*}
	\Cov(X,Y)&=E((X-E(X))(Y-E(Y)))\\
	&=E(XY-E(Y)X-E(X)Y+E(X)E(Y))\\
	&=E(XY)-E(Y)E(X)-E(X)E(Y)+E(X)E(Y)\\
	&=E(XY)-E(X)E(Y)
\end{align*}

We may call $E(XY)$ the \textbf{product moment} of $X$ and $Y$.

When we say \textit{computationally favourable}, we mean that this formulation is useful for computing covariance by hand. It is actually the case that this formulation is numerically unstable and should thus be avoided in computer programs.

Observe that if $E(XY)>E(X)E(Y)$, then $\Cov(X,Y)>0$. In this case, we say that $X$ and $Y$ are \textbf{positively associated}.

Now observe that if $E(XY)<E(X)E(Y)$, then $\Cov(X,Y)<0$. In this case, we say that $X$ and $Y$ are \textbf{negatively associated}.

In the case that $E(XY)=E(X)E(Y)$, then $\Cov(X,Y)=0$. In this case, we say that $X$ and $Y$ are \textbf{not associated}.

We may ask: ``Can covariance tell us how closely two random variables are related?'' The answer to this question is no. The measure of covariance is \textit{not} invariant to scale.

Suppose we would like to compute $\Cov(aX,bY)$ for $a,b\in\mathbb R$. By the linearity of expectation, we find the following.
\begin{align*}
	\Cov(aX,bY)&=E((aX)(bY))-E(aX)E(bY)\\
	&=ab(E(XY)-E(X)E(Y))\\
	&=ab\Cov(X,Y)
\end{align*}

As such, we would like a measure of association between random variables that remains invariant under scaling. This measure is correlation.

\begin{definition}[correlation coefficient]
	Suppose that $(X,Y)$ is a random vector with covariance $\Cov(X,Y)$. Then the \textbf{correlation coefficient} between $X$ and $Y$, denoted $\Corr(X,Y)$, is defined by

	$$\Corr(X,Y)=\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}.$$
\end{definition}

We may also use $\rho_{X,Y}$ or simply $\rho$ to denote the correlation coefficient.

\begin{theorem}[]
	For all jointly distributed random variables $(X,Y)$ where $\Corr(X,Y)$ exists, $\Corr(aX+b,cY+d)=\sgn(ac)\Corr(X,Y)$ for any scalars $a,b,c,d\in\mathbb R$.
\end{theorem}

\begin{proof}
	Recall the definition of the correlation coefficient. $$\Corr(X,Y)=\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}$$

	We compute $\Cov(aX+b,cY+d)$ using the linearity of expectation.

	\begin{align*}
		\Cov(aX+b,cY+d)&=E((aX+b)(cY+d))-E(aX+b)E(cY+d)\\
		&=E(acXY+adX+bcY+bd)-E(aX+b)E(cY+d)\\
		&=acE(XY)+adE(X)+bcE(Y)+bd-(aE(X)+b)(cE(Y)+d)\\
		&=acE(XY)+adE(X)+bcE(Y)+bd-acE(X)E(Y)-adE(X)-bcE(Y)-bd\\
		&=acE(XY)-acE(X)E(Y)\\
		&=ac(E(XY)-E(X)E(Y))\\
		&=ac\Cov(X,Y)
	\end{align*}

	We have that $\Var(aX+b)=a^2\Var(X)$ and $\Var(cY+d)=c^2\Var(Y)$. We substitute these into the definition of the correlation coefficient.

	\begin{align*}
		\Corr(aX+b,cY+d)&=\frac{\Cov(aX+b,cY+d)}{\sqrt{\Var(aX+b)\Var(cY+d)}}\\
		&=\frac{ac\Cov(X,Y)}{\sqrt{a^2\Var(X)c^2\Var(Y)}}\\
		&=\frac{ac\Cov(X,Y)}{|ac|\sqrt{\Var(X)\Var(Y)}}\\
		&=\sgn(ac)\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}\\
		&=\sgn(ac)\Corr(X,Y)
	\end{align*}
\end{proof}

\begin{theorem}[]
	Let $(X,Y)$ be a random vector. Then $\Corr(X,Y)=0$ if and only if $\Cov(X,Y)=0$.
\end{theorem}

\begin{theorem}[]
	For all jointly distributed random variables $(X,Y)$ where $\Corr(X,Y)$ exists, $-1\leq\Corr(X,Y)\leq 1$.
\end{theorem}

\begin{proof}
	We prove the discrete case. That is, when $(X,Y)$ is a discrete random vector.

	Recall the Cauchy-Schwarz inequality:
	$$\left|\sum_{i=1}^na_ib_i\right|\leq\left(\sum_{i=1}^n{a_i}^2\right)^{\frac 12}\left(\sum_{i=1}^n{b_i}^2\right)^{\frac 12}$$

	Consider the following.

	\begin{align*}
		|\Cov(X,Y)|&=\left|\sum(x-E(X))(y-E(Y))p_{X,Y}(xy)\right|\\
		\intertext{Let $a_i=(x-E(X))\sqrt{p_{X,Y}(xy)}$ and $b_i=(y-E(Y))\sqrt{p_{X,Y}(xy)}$.}
		&=\left|\sum a_ib_i\right|\\
		\intertext{We use Cauchy-Schwarz.}
		&\leq\left(\sum a_i^2\right)^{\frac 12}\left(\sum b_i^2\right)^{\frac 12}\\
		&=\left(\sum (x-E(X))^2p_{X,Y}(xy)\right)^{\frac 12}\left(\sum (y-E(Y))^2p_{X,Y}(xy)\right)^{\frac 12}\\
		&=\sqrt{\Var(X)\Var(Y)}
	\end{align*}

	So we have that $|\Cov(X,Y)|\leq \sqrt{\Var(X)\Var(Y)}$. That is,
	$$\left|\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}\right|=|\Corr(X,Y)|\leq 1$$
\end{proof}

\begin{example}[]
	Consider the set up of Example \ref{ex:dice_jointpmf}. Find $\Cov(X,W)$ and $\Corr(X,W)$.
	\textit{Solution.} Recall that $\Cov(X,W)=E(XW)-E(X)E(W)$.
	Recall that the marginal pmf of $W$ from Example \ref{ex:dice_marginalpmf} is $$p_W(w)=\begin{cases}
		\frac{2w-1}{36} & \text{if $w=1,\hdots, 6$}\\
		0 & \text{otherwise}
	\end{cases}.$$

	We also have that the marginal pmf of $X$ is $$p_X(x)=\begin{cases}
		\frac 16 & \text{if $x=1,\hdots, 6$}\\
		0 & \text{otherwise}\end{cases}.$$

	We compute $E(X)$ and $E(W)$.

	\begin{align*}
		E(X)&=\sum_xxp_X(x)\\
		&=\sum_{x=1}^6x\frac 16\\
		&=\frac 16+\frac 26+\cdots+\frac 66\\
		&=\frac{21}6\\
		&=\frac 72
	\end{align*}

	\begin{align*}
		E(W)&=\sum_wwp_W(w)\\
		&=\sum_{w=1}^6w\frac{2w-1}{36}\\
		&=(1)\frac{1}{36}+(2)\frac{3}{36}+\cdots+(6)\frac{11}{36}\\
		&=\frac{161}{36}
	\end{align*}

	We find the product moment of $X$ and $W$ using the joint pmf $p_{X,W}$ defined in Example \ref{ex:dice_jointpmf}.
	\begin{align*}
		E(XW)&=\mathop{\sum\sum}_{(x,w)}xwp_{X,W}(x,w)\\
		&=\sum_{x=1}^6x\sum_{w=1}^6wp_{X,W}(x,w)\\
		&=\frac{154}{9}
	\end{align*}

	We now see the following.
	\begin{align*}
		\Cov(X,W)&=E(XW)-E(X)E(W)\\
		&=\frac{154}{9}-\frac 72\cdot\frac{161}{36}\\
		&=\frac{105}{72}
	\end{align*}

	Recall that $\Corr(X,W)=\frac{\Cov(X,W)}{\sqrt{\Var(X)\Var(W)}}$.

	We compute $\Var(X)$ and $\Var(W)$.

	\begin{align*}
		\Var(X)&=E(X^2)-E(X)^2\\
		&=\sum_xx^2p_X(x)-E(X)^2\\
		&=\sum_{x=1}^6\frac{x^2}{6}-E(X)^2\\
		&=\frac{1^2}{6}+\frac{2^2}{6}+\cdots+\frac{6^2}{6}-\left(\frac 72\right)^2\\
		&=\frac{105}{36}\\
		&=\frac{35}{12}
	\end{align*}

	\begin{align*}
		\Var(W)&=E(W^2)-E(W)^2\\
		&=\sum_ww^2p_W(w)-E(W)^2\\
		&=\sum_{w=1}^6w^2\frac{2w-1}{36}-E(V)^2\\
		&=1^2\cdot\frac{1}{36}+2^2\cdot\frac{3}{36}+\cdots+6^2\cdot\frac{11}{36}-\left(\frac{161}{36}\right)^2\\
		&=\frac{2555}{1296}
	\end{align*}

	Then we have the following.

	\begin{align*}
		\Corr(X,W)&=\frac{\Cov(X,W)}{\sqrt{\Var(X)\Var(W)}}\\
		&=\frac{\frac{105}{72}}{\sqrt{\frac{35}{12}\cdot\frac{2555}{1296}}}\\
		&\approx 0.6082
	\end{align*}
\end{example}

\begin{example}[]
	Consider the setup of Example \ref{ex:jointpdf_cxy}. Find $\Cov(X,Y)$ and $\Corr(X,Y)$.

	\textit{Solution.} Recall that $E(XY)=1$ from Example \ref{ex:jointpdf_E(XY)}. We compute $E(X)$ and $E(Y)$.

	To be completed
\end{example}

\subsection{Conditional distributions}

\begin{definition}[conditional pmf]
	Let $X_1$ and $X_2$ be discrete random variables with joint pmf $p_{X_1,X_2}$. Let $p_{X_1}$ and $p_{X_2}$ denote the marginal pmfs of $X_1$ and $X_2$, respectively. Suppose $x_1$ is such that $p_{X_1}(x_1)>0$. The \textbf{conditional pmf} of $X_2$ given that $X_1=x_1$ is defined by
	$$p_{X_2|X_1}(x_2|x_1)=\frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_1}(x_1)}$$
	where $x_2$ is in the support of $X_2$.
\end{definition}

\begin{definition}[conditional pdf]
	Let $X_1$ and $X_2$ be continuous random variables with joint pdf $f_{X_1,X_2}$. Let $f_{X_1}$ and $f_{X_2}$ denote the marginal pdfs of $X_1$ and $X_2$, respectively. Suppose $x_1$ is such that $f_{X_1}(x_1)>0$. The \textbf{conditional pdf} of $X_2$ given that $X_1=x_1$ is defined by
	$$f_{X_2|X_1}(x_2|x_1)=\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_1}(x_1)}$$
	where $x_2$ is in the support of $X_2$.
\end{definition}

%THIS ENDS MATERIAL COVERED IN LECTURE. THE FOLLOWING HAS BEEN TYPESET TENTATIVELY.
