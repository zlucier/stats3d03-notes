\rhead{\color{main}2 Multivariate distributions}
\section{Multivariate distributions}

We will often want to deal with more than one variable based on the same random experiment.

\begin{definition}[random vector]
	Consider a random experiment with sample space $\Omega$. Let $X_1,X_2:\Omega\to\mathbb R$ be random variables. We say that $(X_1,X_2)$ is a \textbf{random vector}. The support of $(X_1,X_2)$ is the set of ordered pairs $\chi=\{(x_1,x_2):X_1(\omega)=x_1,X_2(\omega)=x_2,\omega\in \Omega\}$.
\end{definition}

Of particular interest are events of the form $(X_1\leq x_1)\cap(X_2\leq x_2)$ for $(x_1,x_2)\in\mathbb R^2$.

\begin{definition}[joint cdf]
	Suppose that $(X_1,X_2)$ is a random vector. The \textbf{joint cdf} of $(X_1,X_2)$, is the function $F_{X_1,X_2}$ defined as $$F_{X_1,X_2}(x_1,x_2)=P(X_1\leq x_1,X_2\leq x_2).$$
\end{definition}

From here we can extend the univariate case for the probability over intervals to rectangular subsets of $\mathbb R^2$.

\begin{theorem}[Rectangular probability formula]
	Suppose that the random vector $(X_1,X_2)$ has joint cdf $F_{X_1,X_2}$ and let $a_1,b_1,a_2,b_2\in\mathbb R$ be such that $a_1<b_1$ and $a_2<b_2$. Then $$P\left((X_1,X_2)\in[a_1,b_1]\times[a_2,b_2]\right)=F_{X_1,X_2}(b_1,b_2)-F_{X_1,X_2}(a_1,b_2)-F_{X_1,X_2}(b_1,a_2)+F_{X_1,X_2}(a_1,a_2).$$
\end{theorem}

Recall that, in general, random variables can be of the discrete type or of the continuous type. We extend this idea to random vectors.

A random vector $(X_1,X_2)$ is said to be \textbf{discrete} if its support $\chi$ is countable. In this case both $X_1$ and $X_2$ are discrete random variables. It thus makes sense to define pmfs for discrete random vectors.

\begin{definition}[joint pmf]
	Let $(X_1,X_2)$ be a discrete random vector. Then the \textbf{joint pmf} of $(X_1,X_2)$, is the function $p_{X_1,X_2}$ given by

	$$p_{X_1,X_2}(x_1,x_2)=P(X_1=x_1,X_2=x_2).$$
\end{definition}

As in the univariate case, this joint pmf satisfies the following properties.
\begin{enumerate}
	\item $0\leq p_{X_1,X_2}(x_1,x_2)\leq 1$ for all $(x_1,x_2)\in\chi$.
	\item $\sum_{(x_1,x_2)\in\chi}p_{X_1,X_2}(x_1,x_2)=1$.
\end{enumerate}

\begin{example}[]
	Suppose two dice are rolled. Let $X$ denote the number of dots facing up on the first die and $Y$ the number of dots on the second die. Also, let $W=\max(X,Y)$. We would like to find the joint pmf of the random vector $(X,W)$ and the probability that $X=W$.

	\textit{Solution.} Let $\chi$ denote the support of $(X,W)$. It is clear that $\chi=\{1,\hdots,6\}\times\{1,\hdots,6\}$.

	The joint pmf $p_{X,W}$ of $(X,W)$ can be summarized by the following table.

	\begin{center}
	%\rowcolors{2}{white}{main!10}
	\begin{tabular}{c | c c c c c c}
		$W/X$ & 1 & 2 & 3 & 4 & 5 & 6\\\hline
		1 & $\frac{1}{36}$ & 0 & 0 & 0 & 0 & 0\\
		2 & $\frac{1}{36}$ & $\frac{2}{36}$ & 0 & 0 & 0 & 0\\
		3 & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{3}{36}$ & 0 & 0 & 0\\
		4 & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{4}{36}$ & 0 & 0\\
		5 & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{5}{36}$ & 0\\
		6 & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{6}{36}$
	\end{tabular}
	\end{center}

	It is clearly the case that $0\leq p_{X,W}(x,w)\leq 1$ and $\sum_{(x,w)\in\chi}p_{X,W}(x,w)=1$.

	We can now find the probability that $X=W$. That is, that the first die has the larger number of dots. We sum along the diagonal of the above table.
	\begin{align*}
		\sum_{\substack{(x,w)\in\chi\\x=w}}p_{X,W}(x,w)&=\frac 1{36}+\frac{2}{36}+\cdots+\frac{6}{36}\\
		&=\frac{7}{12}
	\end{align*}

\end{example}

If the joint cdf $F_{X_1,X_2}$ of a random vector $(X_1,X_2)$ is continuous, then we say that $(X_1,X_2)$ is \textbf{continuous}. Similarly to a joint pmf, we can also define a joint pdf.

\begin{definition}[joint pdf]
	Let $(X_1,X_2)$ be a continuous random vector. Then the \textbf{joint pdf} of $(X_1,X_2)$ is the function $f_{X_1,X_2}$ satisfying
	$$F_{X_1,X_2}(x_1,x_2)=\int_{-\infty}^{x_2}\int_{-\infty}^{x_1}f_{X_1,X_2}(u,v)\dd u\dd v$$
	for all $(x_1,x_2)\in\mathbb R^2$.
\end{definition}

As in the univariate case, the joint pdf satisfies the following properties.
\begin{enumerate}
	\item $f_{X_1,X_2}(x_1,x_2)\geq 0$ for all $(x_1,x_2)\in\mathbb R^2$.
	\item $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{X_1,X_2}(u,v)\dd u\dd v=1$.
\end{enumerate}

Often times, we will want to obtain the distributions of the random variables $X_1$ and $X_2$ from the joint distribution of $(X_1,X_2)$.

Given the above setup, we may obtain the \textbf{marginal cdf} of $X_1$ from the following equivalent formulations.
\begin{align*}
	F_{X_1}&=P(X_1\leq x_1)\\
	&=P(X_1\leq x_1,-\infty<X_2<\infty)\\
	&=\lim_{x_2\to\infty}P(X_1\leq x_1,X_2\leq x_2)\\
	&=\lim_{x_2\to\infty} F_{X_1,X_2}(x_1,x_2)
\end{align*}

We may also find \textbf{marginal pmfs} and \textbf{marginal pdfs}.

If $(X_1,X_2)$ is a discrete random vector, then
\begin{align*}
	p_{X_1}(x_1)&=P(X_1=x_1)\\
	&=\sum_{x_2}p_{X_1,X_2}(x_1,x_2)
\end{align*}

If $(X_1,X_2)$ is a continuous random vector, then
\begin{align*}
	f_{X_1}(x_1)&=\int_{-\infty}^{\infty}f_{X_1,X_2}(x_1,x_2)\dd{x_2}\\
\end{align*}

\begin{example}[]
	We revisit the previous example. We would like to find the marginal pmf of $W$.

	\textit{Solution.} We apply the definition.
	\begin{center}
	\begin{tabular}{c | c}
		$W$ & $p_W(w)$\\ \hline
		1 & $\frac{1}{36}$\\
		2 & $\frac{3}{36}$\\
		3 & $\frac{5}{36}$\\
		4 & $\frac{7}{36}$\\
		5 & $\frac{9}{36}$\\
		6 & $\frac{11}{36}$
	\end{tabular}
	\end{center}
\end{example}


\begin{example}[]
	Consider the joint pdf of $(X,Y)$ to be
	$$f_{X,Y}(x,y)=\begin{cases}
		cxy^2 & \text{for $0\leq x\leq 2,0\leq y\leq 1$}\\
		0 & \text{otherwise}
	\end{cases}$$
	We would like to find the value of $c$ such that $f_{X,Y}$ is a valid pdf. We would then like to find the marginal pdfs.

	For $f_{X,Y}$ to be valid, we require it to be non-negative and it must integrate to 1. The second property will be used to determine $c$.

	\begin{align*}
		\int_0^1\int_0^2 cxy^2\dd x\dd y & =1\\
		c\int_0^1y^2\int_0^2 x\dd x\dd y & =1\\
		c\left(\frac 13\right)(2)&=1\\
		\frac 23 c&=1\\
		c&=\frac 32
	\end{align*}

	Such a $c$ makes $f_{X,Y}$ a valid pdf.

	We first find the marginal pdf of $X$.

	\begin{align*}
		f_X(x)&=\int_0^1\frac 32xy^2\dd y\\
		&=\frac 32x\int_0^1y^2\dd y\\
		&=\frac x2
	\end{align*}

	So $f_X(x)=\begin{cases}
		\frac x 2 & \text{for $0\leq x\leq 2$}\\
		0 & \text{otherwise}
	\end{cases}$

	We leave the marginal pdf of $Y$ as an exercise.
\end{example}

Expectations in the multivariate case are easily extended from the univariate case to random vectors.

\begin{theorem}[Law of the unconscious statistician (multivariate)]
	Let $(X_1,X_2)$ be a random vector and let $Y=g(X_1,X_2)$ for some real-valued function $g$. Then $Y$ is a random variable and we have the following.
	\begin{enumerate}[label=\color{main}(\alph*)]
		\item Suppose $(X_1,X_2)$ is discrete with pmf $p_{X_1,X_2}(x_1,x_2)$. If $$\sum_{x_1}\sum_{x_2}|g(x_1,x_2)|p_{X_1,X_2}(x_1,x_2)$$ is finite, then the expectation of $Y$ exists and is given by $$E(Y)=\sum_{x_1}\sum_{x_2}g(x_1,x_2)p_{X_1,X_2}(x_1,x_2).$$
		\item Suppose $(X_1,X_2)$ is continuous with pdf $f_{X_1,X_2}(x_1,x_2)$. If $$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|g(x_1,x_2)|f_{X_1,X_2}(x_1,x_2)\dd{x_1}\dd{x_2}$$ is finite, then the expectation of $Y$ exists and is given by $$E(Y)=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x_1,x_2)f_{X_1,X_2}(x_1,x_2)\dd{x_1}\dd{x_2}.$$
	\end{enumerate}
\end{theorem}

We can now that expectation is a linear operator.

\begin{theorem}[Linearity of expectation]
	Let $(X_1,X_2)$ be a random vector and let $Y_1=g_1(X_1,X_2)$ and $Y_2=g_2(X_1,X_2)$ be random variables for some real-valued functions $g_1$ and $g_2$. Suppose that both $E(Y_1)$ and $E(Y_2)$ exist. Then for all $k_1,k_2\in\mathbb R$,
	$$E(k_1Y_1+k_2Y_2)=k_1E(Y_1)+k_2E(Y_2).$$
\end{theorem}

\begin{proof}
	We prove for the discrete case.

	We show absolute convergence using the triangle inequality.
	\begin{align*}
		\sum_{x_1}\sum_{x_2}|k_1g_1(x_1,x_2)+k_2g_2(x_1,x_2)|p_{X_1,X_2}(x_1,x_2)&\leq |k_1|\sum_{x_1}\sum_{x_2}|g_1(x_1,x_2)|p_{X_1,X_2}(x_1,x_2)\\
		&\quad+|k_2|\sum_{x_1}\sum_{x_2}|g_2(x_1,x_2)|p_{X_1,X_2}(x_1,x_2)
	\end{align*}

	But $E(Y_1)$ and $E(Y_2)$ exist, so the above is finite and $E(k_1Y_1+k_2Y_2)$ exists.
	\begin{align*}
		E(k_1Y_1+k_2Y_2)&=\sum_{x_1}\sum_{x_2}(k_1g_1(x_1,x_2)+k_2g_2(x_1,x_2))p_{X_1,X_2}(x_1,x_2)\\
		&=k_1\sum_{x_1}\sum_{x_2}g_1(x_1,x_2)p_{X_1,X_2}(x_1,x_2)+k_2\sum_{x_1}\sum_{x_2}g_2(x_1,x_2)p_{X_1,X_2}(x_1,x_2)\\
		&=k_1E(Y_1)+k_2E(Y_2)
	\end{align*}
\end{proof}
