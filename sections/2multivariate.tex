\rhead{\color{main}2 Multivariate distributions}
\section{Multivariate distributions}

We will often want to deal with more than one variable based on the same random experiment.

\begin{definition}[random vector]
	Consider a random experiment with sample space $\Omega$. Let $X_1,X_2:\Omega\to\mathbb R$ be random variables. We say that $(X_1,X_2)$ is a \textbf{random vector}. The support of $(X_1,X_2)$ is the set of ordered pairs $\chi=\{(x_1,x_2):X_1(\omega)=x_1,X_2(\omega)=x_2,\omega\in \Omega\}$.
\end{definition}

\subsection{Joint distributions}

Of particular interest are events of the form $(X_1\leq x_1)\cap(X_2\leq x_2)$ for $(x_1,x_2)\in\mathbb R^2$.

\begin{definition}[joint cdf]
	Suppose that $(X_1,X_2)$ is a random vector. The \textbf{joint cdf} of $(X_1,X_2)$, is the function $F_{X_1,X_2}$ defined as $$F_{X_1,X_2}(x_1,x_2)=P(X_1\leq x_1,X_2\leq x_2).$$
\end{definition}

From here we can extend the univariate case for the probability over intervals to rectangular subsets of $\mathbb R^2$.

\begin{theorem}[Rectangular probability formula]
	Suppose that the random vector $(X_1,X_2)$ has joint cdf $F_{X_1,X_2}$ and let $a_1,b_1,a_2,b_2\in\mathbb R$ be such that $a_1<b_1$ and $a_2<b_2$. Then $$P\left((X_1,X_2)\in[a_1,b_1]\times[a_2,b_2]\right)=F_{X_1,X_2}(b_1,b_2)-F_{X_1,X_2}(a_1,b_2)-F_{X_1,X_2}(b_1,a_2)+F_{X_1,X_2}(a_1,a_2).$$
\end{theorem}

Recall that, in general, random variables can be of the discrete type or of the continuous type. We extend this idea to random vectors.

A random vector $(X_1,X_2)$ is said to be \textbf{discrete} if its support $\chi$ is countable. In this case both $X_1$ and $X_2$ are discrete random variables. It thus makes sense to define pmfs for discrete random vectors.

\begin{definition}[joint pmf]
	Let $(X_1,X_2)$ be a discrete random vector. Then the \textbf{joint pmf} of $(X_1,X_2)$, is the function $p_{X_1,X_2}$ given by

	$$p_{X_1,X_2}(x_1,x_2)=P(X_1=x_1,X_2=x_2).$$
\end{definition}

As in the univariate case, this joint pmf satisfies the following properties.
\begin{enumerate}
	\item $0\leq p_{X_1,X_2}(x_1,x_2)\leq 1$ for all $(x_1,x_2)\in\chi$.
	\item $\sum_{(x_1,x_2)\in\chi}p_{X_1,X_2}(x_1,x_2)=1$.
\end{enumerate}

\begin{example}[]\label{ex:dice_jointpmf}
	Suppose two dice are rolled. Let $X$ denote the number of dots facing up on the first die and $Y$ the number of dots on the second die. Also, let $W=\max(X,Y)$. We would like to find the joint pmf of the random vector $(X,W)$ and the probability that $X=W$.

	\textit{Solution.} Let $\chi$ denote the support of $(X,W)$. It is clear that $\chi=\{1,\hdots,6\}\times\{1,\hdots,6\}$.

	The joint pmf $p_{X,W}(x,w)$ of $(X,W)$ can be summarized by the following table.

	\begin{center}
	\begin{tabular}{c | c c c c c c}
		$p_{X,W}(x,w)$ & $x=1$ & $x=2$ & $x=3$ & $x=4$ & $x=5$ & $x=6$\\\hline
		$w=1$ & $\frac{1}{36}$ & 0 & 0 & 0 & 0 & 0\\
		$w=2$ & $\frac{1}{36}$ & $\frac{2}{36}$ & 0 & 0 & 0 & 0\\
		$w=3$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{3}{36}$ & 0 & 0 & 0\\
		$w=4$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{4}{36}$ & 0 & 0\\
		$w=5$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{5}{36}$ & 0\\
		$w=6$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{6}{36}$
	\end{tabular}
	\end{center}

	It is clearly the case that $0\leq p_{X,W}(x,w)\leq 1$ and $\sum_{(x,w)\in\chi}p_{X,W}(x,w)=1$.

	We can now find the probability that $X=W$. That is, that the first die has the larger number of dots. We sum along the diagonal of the above table.
	\begin{align*}
		\sum_{\substack{(x,w)\in\chi\\x=w}}p_{X,W}(x,w)&=\frac 1{36}+\frac{2}{36}+\cdots+\frac{6}{36}\\
		&=\frac{7}{12}
	\end{align*}

\end{example}

If the joint cdf $F_{X_1,X_2}$ of a random vector $(X_1,X_2)$ is continuous, then we say that $(X_1,X_2)$ is \textbf{continuous}. Similarly to a joint pmf, we can also define a joint pdf.

\begin{definition}[joint pdf]
	Let $(X_1,X_2)$ be a continuous random vector. Then the \textbf{joint pdf} of $(X_1,X_2)$ is the function $f_{X_1,X_2}$ satisfying
	$$F_{X_1,X_2}(x_1,x_2)=\int_{-\infty}^{x_2}\int_{-\infty}^{x_1}f_{X_1,X_2}(u,v)\dd u\dd v$$
	for all $(x_1,x_2)\in\mathbb R^2$.
\end{definition}

As in the univariate case, the joint pdf satisfies the following properties.
\begin{enumerate}
	\item $f_{X_1,X_2}(x_1,x_2)\geq 0$ for all $(x_1,x_2)\in\mathbb R^2$.
	\item $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{X_1,X_2}(u,v)\dd u\dd v=1$.
\end{enumerate}

Often times, we will want to obtain the distributions of the random variables $X_1$ and $X_2$ from the joint distribution of $(X_1,X_2)$.

We may obtain the \textbf{marginal cdf} of $X_1$ from the following equivalent formulations.
\begin{align*}
	F_{X_1}(x_1)&=P(X_1\leq x_1)\\
	&=P(X_1\leq x_1,-\infty<X_2<\infty)\\
	&=\lim_{x_2\to\infty}P(X_1\leq x_1,X_2\leq x_2)\\
	&=\lim_{x_2\to\infty} F_{X_1,X_2}(x_1,x_2)
\end{align*}

We may also find \textbf{marginal pmfs} and \textbf{marginal pdfs}.

If $(X_1,X_2)$ is a discrete random vector, then
\begin{align*}
	p_{X_1}(x_1)&=P(X_1=x_1)\\
	&=\sum_{x_2}p_{X_1,X_2}(x_1,x_2)
\end{align*}

If $(X_1,X_2)$ is a continuous random vector, then
\begin{align*}
	f_{X_1}(x_1)&=\int_{-\infty}^{\infty}f_{X_1,X_2}(x_1,x_2)\dd{x_2}\\
\end{align*}

\begin{example}[]\label{ex:dice_marginalpmf}
	We revisit the setup of Example \ref{ex:dice_jointpmf}. We would like to find the marginal pmf of $W$.

	\textit{Solution.} We apply the definition.
	\begin{center}
	\begin{tabular}{c | c}
		$w$ & $p_W(w)$\\ \hline
		1 & $\frac{1}{36}$\\
		2 & $\frac{3}{36}$\\
		3 & $\frac{5}{36}$\\
		4 & $\frac{7}{36}$\\
		5 & $\frac{9}{36}$\\
		6 & $\frac{11}{36}$
	\end{tabular}
	\end{center}

	We may write $$p_W(w)=\begin{cases}
		\frac{2w-1}{36} & \text{if $w=1,\hdots, 6$}\\
		0 & \text{otherwise}
	\end{cases}.$$
\end{example}


\begin{example}[]\label{ex:jointpdf_cxy}
	Consider the joint pdf of $(X,Y)$ to be
	$$f_{X,Y}(x,y)=\begin{cases}
		cxy^2 & \text{for $0\leq x\leq 2,0\leq y\leq 1$}\\
		0 & \text{otherwise}
	\end{cases}$$
	Find the value of $c$ such that $f_{X,Y}$ is a valid pdf and find the marginal pdfs of $X$ and $Y$.

	\textit{Solution.} For $f_{X,Y}$ to be valid, we require it to be non-negative and it must integrate to 1. The second property will be used to determine $c$.

	\begin{align*}
		\int_0^1\int_0^2 cxy^2\dd x\dd y & =1\\
		c\int_0^1y^2\int_0^2 x\dd x\dd y & =1\\
		c\int_0^1y^2\left[\frac{x^2}{2}\right]_0^2\dd y & =1\\
		2c\int_0^1y^2\dd y & =1\\
		2c\left[\frac{y^3}{3}\right]_0^1 & =1\\
		\frac 23 c&=1\\
		c&=\frac 32
	\end{align*}

	Such a $c=\frac 32$ makes $f_{X,Y}$ a valid pdf.

	We first find the marginal pdf of $X$.

	\begin{align*}
		f_X(x)&=\int_0^1\frac 32xy^2\dd y\\
		&=\frac 32x\int_0^1y^2\dd y\\
		&=\frac 32x\left[\frac{y^3}{3}\right]_0^1\\
		&=\frac x2
	\end{align*}

	So $f_X(x)=\begin{cases}
		\frac x 2 & \text{for $0\leq x\leq 2$}\\
		0 & \text{otherwise}
	\end{cases}$.

	We now find the marginal pdf of $Y$.

	\begin{align*}
		f_Y(y)&=\int_0^2\frac 32xy^2\dd x\\
		&=\frac 32y^2\int_0^2x\dd x\\
		&=\frac 32y^2\left[\frac{x^2}{2}\right]_0^2\\
		&=3y^2
	\end{align*}

	So $f_Y(x)=\begin{cases}
		3y^2 & \text{for $0\leq y\leq 1$}\\
		0 & \text{otherwise}
	\end{cases}$.
\end{example}

\subsection{Expectation}

Expectations in the multivariate case are easily extended from the univariate case to random vectors.

\begin{theorem}[Law of the unconscious statistician (multivariate)]\label{thm:lotus2}
	Let $(X_1,X_2)$ be a random vector and let $Y=g(X_1,X_2)$ for some real-valued function $g$. Then $Y$ is a random variable and we have the following.
	\begin{enumerate}[label=\color{main}(\alph*)]
		\item Suppose $(X_1,X_2)$ is discrete with pmf $p_{X_1,X_2}(x_1,x_2)$. If $$\sum_{x_1}\sum_{x_2}|g(x_1,x_2)|p_{X_1,X_2}(x_1,x_2)$$ is finite, then the expectation of $Y$ exists and is given by $$E(Y)=\sum_{x_1}\sum_{x_2}g(x_1,x_2)p_{X_1,X_2}(x_1,x_2).$$
		\item Suppose $(X_1,X_2)$ is continuous with pdf $f_{X_1,X_2}(x_1,x_2)$. If $$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|g(x_1,x_2)|f_{X_1,X_2}(x_1,x_2)\dd{x_1}\dd{x_2}$$ is finite, then the expectation of $Y$ exists and is given by $$E(Y)=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x_1,x_2)f_{X_1,X_2}(x_1,x_2)\dd{x_1}\dd{x_2}.$$
	\end{enumerate}
\end{theorem}

We often abbreviate this theorem LOTUS.

We can now show that expectation is a linear operator.

\begin{theorem}[Linearity of expectation]
	Let $(X_1,X_2)$ be a random vector and let $Y_1=g_1(X_1,X_2)$ and $Y_2=g_2(X_1,X_2)$ be random variables for some real-valued functions $g_1$ and $g_2$. Suppose that both $E(Y_1)$ and $E(Y_2)$ exist. Then for any $k_1,k_2\in\mathbb R$,
	$$E(k_1Y_1+k_2Y_2)=k_1E(Y_1)+k_2E(Y_2).$$
\end{theorem}

\begin{proof}
	We prove for the discrete case.

	We show absolute convergence using the triangle inequality.
	\begin{align*}
		\sum_{x_1}\sum_{x_2}|k_1g_1(x_1,x_2)+k_2g_2(x_1,x_2)|p_{X_1,X_2}(x_1,x_2)&\leq |k_1|\sum_{x_1}\sum_{x_2}|g_1(x_1,x_2)|p_{X_1,X_2}(x_1,x_2)\\
		&\quad+|k_2|\sum_{x_1}\sum_{x_2}|g_2(x_1,x_2)|p_{X_1,X_2}(x_1,x_2)
	\end{align*}

	But $E(Y_1)$ and $E(Y_2)$ exist, so the above is finite and $E(k_1Y_1+k_2Y_2)$ exists.
	\begin{align*}
		E(k_1Y_1+k_2Y_2)&=\sum_{x_1}\sum_{x_2}(k_1g_1(x_1,x_2)+k_2g_2(x_1,x_2))p_{X_1,X_2}(x_1,x_2)\\
		&=k_1\sum_{x_1}\sum_{x_2}g_1(x_1,x_2)p_{X_1,X_2}(x_1,x_2)+k_2\sum_{x_1}\sum_{x_2}g_2(x_1,x_2)p_{X_1,X_2}(x_1,x_2)\\
		&=k_1E(Y_1)+k_2E(Y_2)
	\end{align*}
\end{proof}

\begin{example}[]
	Consider the random vector $(X,Y)$ with joint pmf $p_{X,Y}(x,y)$ defined as follows.

	\begin{center}
	\begin{tabular}{c | c c c}
		$p_{X,Y}(x,y)$ & $y=1$ & $y=2$ & $y=3$\\ \hline
		$x=0$ & $\frac{1}{18}$ & $\frac{1}{9}$ & $\frac{1}{18}$\\
		$x=1$ & $\frac{1}{9}$ & $\frac{1}{9}$ & $\frac{1}{18}$\\
		$x=2$ & $\frac{2}{9}$ & $\frac{1}{18}$ & $\frac{2}{9}$
	\end{tabular}
	\end{center}

	Find $E(XY)$.

	\textit{Solution.} Let $g(X,Y)=XY$. We use Theorem \ref{thm:lotus2} (LOTUS).
	\begin{align*}
		E(X,Y)&=\sum_{x}\sum_{y}xyp_{X,Y}(xy)\\
		&=(0)(1)\frac{1}{18}+(0)(2)\frac{1}{9}+(0)(3)\frac{1}{18}\\
		&\quad +(1)(1)\frac{1}{9}+(1)(2)\frac{1}{9}+(1)(3)\frac{1}{18}\\
		&\quad +(2)(1)\frac{2}{9}+(2)(2)\frac{1}{18}+(2)(3)\frac{9}{9}\\
		&=\frac{45}{18}
	\end{align*}
\end{example}

\begin{example}[]\label{ex:jointpdf_E(XY)}
	Recall the setup of Example \ref{ex:jointpdf_cxy}. Find $E(XY)$.

	\textit{Solution.}

	\begin{align*}
		E(XY)&=\int_0^1\int_0^2(xy)\frac 32xy^2\dd x\dd y\\
		&=\frac 32\int_0^1y^3\int_0^2x^2\dd x\dd y\\
		&=\frac 32\int_0^1y^3\left[\frac{x^3}3\right]_0^2\dd y\\
		&=4\int_0^1y^3\dd y\\
		&=4\left[\frac{y^4}{4}\right]_0^1\\
		&=4\cdot\frac 14\\
		&=1
	\end{align*}
\end{example}

\subsection{Joint measures}

For an event $B$ in the support of the discrete random vector $(X,Y)$ with pmf $p_{X,Y}$,
$$P((X,Y)\in B)=\sum_{(x,y)\in B}p_{X,Y}(x,y).$$

For an event $A$ in the support of the continuous random vector $(X,Y)$ with pdf $f_{X,Y}$,
$$P((X,Y)\in A)=\iint_Af_{X,Y}(x,y)\dd x\dd y.$$

\begin{example}[]
	Recall the setup of Example \ref{ex:jointpdf_cxy}. Find $P(X+Y\leq 2)$.

	\textit{Solution.} Let $A$ denote the event $X+Y\leq 2$. Then
	\begin{align*}
		P((X,Y)\in A)=\int\int_A\frac 32xy^2\dd y\dd x.
	\end{align*}


	The support of $(X,Y)$ is $\chi=\{(x,y):0\leq x\leq 2,0\leq y\leq 1\}$ and the set of points under and on the line $X+Y=2$ is $\{(x,y)\in\mathbb R^2:y\leq 2-x\}$. As such, $A$ is the intersection of these sets and, notably, $A=\{(x,y)\in\chi:y\leq 2-x\}$. We illustrate these sets in the following plot.

	\begin{center}
	\begin{tikzpicture}
	\begin{axis}[
	GridPlot1,
	xmin=0,
	xmax=2.25,
	ymin=0,
	ymax=2.25,
	xtick={0,1,2},
	ytick={0,1,2},
	xlabel={$X$},
	ylabel={$Y$},
    scale=0.75
	%axis equal
	]
	\path[name path=axis] (axis cs:0,0) -- (axis cs:2,0);
	\path[name path=y1] (0,1) -- (2,1);

	\node at (1.5,1.25) {\color{indigo}$\chi$};

	 \addplot[
        thick,
        color=indigo,
        fill opacity=0.2]
    fill between[
        of=y1 and axis,
        soft clip={domain=0:2},
    ];

    \addplot[name path=f,domain=0:2,main,thick] {2-x};

	\addplot[
        thick,
        %color=blue,
        fill=main, 
        fill opacity=0.2]
    fill between[
        of=f and axis,
        soft clip={domain=0:2},
    ];

	\end{axis}
	\end{tikzpicture}
	\end{center}
	We can write $A$ as a disjoint union $A=A_1\cup A_2$, as shown in the following plot.

	\begin{center}
	\begin{tikzpicture}
	\begin{axis}[
	GridPlot1,
	xmin=0,
	xmax=2.25,
	ymin=0,
	ymax=2.25,
	xtick={0,1,2},
	ytick={0,1,2},
	xlabel={$X$},
	ylabel={$Y$},
    scale=0.75
	%axis equal
	]
	\path[name path=axis] (axis cs:0,0) -- (axis cs:2,0);
	\path[name path=y1] (0,1) -- (2,1);

	 \addplot[
        color=pink,
        fill opacity=0.2]
    fill between[
        of=y1 and axis,
        soft clip={domain=0:1},
    ];

    \addplot[name path=f,domain=0:2,main,thick] {2-x};

	\addplot[
        color=green,
        fill opacity=0.2]
    fill between[
        of=f and axis,
        soft clip={domain=1:2},
    ];

    \node at (0.5,0.5) {\color{pink}$A_1$};
    \node at (1.35,0.35) {\color{green}$A_2$};

	\end{axis}
	\end{tikzpicture}
	\end{center}

	We see that $A_1=\{(x,y):0\leq x\leq y\leq 1\}$ and $A_2=\{(x,y):1\leq x\leq 2,0\leq y\leq 2-x\}$. So we can compute the required probability as follows.

	$$P((X,Y)\in A)=\iint_Af_{X,Y}(x,y)\dd y\dd x=\iint_{A_1}f_{X,Y}(x,y)\dd y\dd x+\iint_{A_2}f_{X,Y}(x,y)\dd y\dd x$$

	We solve each integral.

	\begin{align*}
	\iint_{A_1}f_{X,Y}(x,y)\dd y\dd x&=\int_0^1\int_0^1\frac 32xy^2\dd y\dd x\\
		&=\frac 32\int_0^1x\int_0^1y^2\dd y\dd x\\
		&=\frac 32\int_0^1x\frac 13\dd x\\
		&=\frac 12\int_0^1x\dd x\\
		&=\frac 12\cdot\frac 12\\
		&=\frac 14
	\end{align*}

	\begin{align*}
		\iint_{A_2}f_{X,Y}(x,y)\dd y\dd x&=\int_1^2\int_{0}^{2-x}\frac 32xy^2\dd y\dd x\\
		&=\frac 32\int_1^2x\int_{0}^{2-x}y^2\dd y\dd x\\
		&=\frac 32\int_1^2x\frac{(2-x)^3}{3}\dd y\dd x\\
		&=\frac 12\int_1^2x (2-x)^3\dd y\dd x\\
		&=\frac 12\int_1^2-x^4+6x^3-12x^2+8x\dd x\\
		&=\frac 12\cdot\frac 3{10}\\
		&=\frac 3{20}
	\end{align*}

	Adding the two integrals, $P(X+Y\leq 2)=\frac 14+\frac 3{20}=\frac 8{20}$.
\end{example}

\begin{definition}[covariance]
	Suppose $(X,Y)$ is a random vector and that $E(X)$ and $E(Y)$ exist. The \textbf{covariance} of $(X,Y)$, denoted $\Cov(X,Y)$, is defined by the expectation

	$$\Cov(X,Y)=E((X-E(X))(Y-E(Y))).$$
\end{definition}

We may obtain a computationally favourable formulation of covariance using the linearity of expectation.
\begin{align*}
	\Cov(X,Y)&=E((X-E(X))(Y-E(Y)))\\
	&=E(XY-E(Y)X-E(X)Y+E(X)E(Y))\\
	&=E(XY)-E(Y)E(X)-E(X)E(Y)+E(X)E(Y)\\
	&=E(XY)-E(X)E(Y)
\end{align*}

We may call $E(XY)$ the \textbf{product moment} of $X$ and $Y$.

When we say \textit{computationally favourable}, we mean that this formulation is useful for computing covariance by hand. It is actually the case that this formulation is numerically unstable and should thus be avoided in computer programs.

Observe that if $E(XY)>E(X)E(Y)$, then $\Cov(X,Y)>0$. In this case, we say that $X$ and $Y$ are \textbf{positively associated}.

Now observe that if $E(XY)<E(X)E(Y)$, then $\Cov(X,Y)<0$. In this case, we say that $X$ and $Y$ are \textbf{negatively associated}.

In the case that $E(XY)=E(X)E(Y)$, then $\Cov(X,Y)=0$. In this case, we say that $X$ and $Y$ are \textbf{not associated}.

We may ask: ``Can covariance tell us how closely two random variables are related?'' The answer to this question is no. The measure of covariance is \textit{not} invariant to scale.

Suppose we would like to compute $\Cov(aX,bY)$ for $a,b\in\mathbb R$. By the linearity of expectation, we find the following.
\begin{align*}
	\Cov(aX,bY)&=E((aX)(bY))-E(aX)E(bY)\\
	&=ab(E(XY)-E(X)E(Y))\\
	&=ab\Cov(X,Y)
\end{align*}

As such, we would like a measure of association between random variables that remains invariant under scaling. This measure is correlation.

\begin{definition}[correlation coefficient]
	Suppose that $(X,Y)$ is a random vector with covariance $\Cov(X,Y)$. Then the \textbf{correlation coefficient} between $X$ and $Y$, denoted $\Corr(X,Y)$, is defined by

	$$\Corr(X,Y)=\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}.$$
\end{definition}

We may also use $\rho_{X,Y}$ or simply $\rho$ to denote the correlation coefficient.

\begin{theorem}[]
	For all jointly distributed random variables $(X,Y)$ where $\Corr(X,Y)$ exists, $\Corr(aX+b,cY+d)=\sgn(ac)\Corr(X,Y)$ for any scalars $a,b,c,d\in\mathbb R$ with $a,c\neq 0$.
\end{theorem}

\begin{proof}
	Recall the definition of the correlation coefficient. $$\Corr(X,Y)=\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}$$

	We compute $\Cov(aX+b,cY+d)$ using the linearity of expectation.

	\begin{align*}
		\Cov(aX+b,cY+d)&=E((aX+b)(cY+d))-E(aX+b)E(cY+d)\\
		&=E(acXY+adX+bcY+bd)-E(aX+b)E(cY+d)\\
		&=acE(XY)+adE(X)+bcE(Y)+bd-(aE(X)+b)(cE(Y)+d)\\
		&=acE(XY)+adE(X)+bcE(Y)+bd-acE(X)E(Y)-adE(X)-bcE(Y)-bd\\
		&=acE(XY)-acE(X)E(Y)\\
		&=ac(E(XY)-E(X)E(Y))\\
		&=ac\Cov(X,Y)
	\end{align*}

	We have that $\Var(aX+b)=a^2\Var(X)$ and $\Var(cY+d)=c^2\Var(Y)$. We substitute these into the definition of the correlation coefficient.

	\begin{align*}
		\Corr(aX+b,cY+d)&=\frac{\Cov(aX+b,cY+d)}{\sqrt{\Var(aX+b)\Var(cY+d)}}\\
		&=\frac{ac\Cov(X,Y)}{\sqrt{a^2\Var(X)c^2\Var(Y)}}\\
		&=\frac{ac\Cov(X,Y)}{|ac|\sqrt{\Var(X)\Var(Y)}}\\
		&=\sgn(ac)\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}\\
		&=\sgn(ac)\Corr(X,Y)
	\end{align*}
\end{proof}

\begin{theorem}[]
	Let $(X,Y)$ be a random vector. Then $\Corr(X,Y)=0$ if and only if $\Cov(X,Y)=0$.
\end{theorem}

\begin{theorem}[]
	For all jointly distributed random variables $(X,Y)$ where $\Corr(X,Y)$ exists, $-1\leq\Corr(X,Y)\leq 1$.
\end{theorem}

\begin{proof}
	We prove the discrete case. That is, when $(X,Y)$ is a discrete random vector.

	Recall the Cauchy-Schwarz inequality:
	$$\left|\sum ab\right|\leq\left(\sum{a}^2\right)^{\frac 12}\left(\sum{b}^2\right)^{\frac 12}$$

	Suppose $(X,Y)$ has support $\chi$

	\begin{align*}
		|\Cov(X,Y)|&=\left|\sum_{(x,y)\in\chi}(x-E(X))(y-E(Y))p_{X,Y}(x,y)\right|\\
		\intertext{Let $a=(x-E(X))\sqrt{p_{X,Y}(x,y)}$ and $b=(y-E(Y))\sqrt{p_{X,Y}(x,y)}$.}
		&=\left|\sum_{(x,y)\in\chi} ab\right|\\
		\intertext{We use Cauchy-Schwarz.}
		&\leq\left(\sum_{(x,y)\in\chi} a^2\right)^{\frac 12}\left(\sum_{(x,y)\in\chi} b^2\right)^{\frac 12}\\
		&=\left(\sum_{(x,y)\in\chi} (x-E(X))^2p_{X,Y}(x,y)\right)^{\frac 12}\left(\sum_{(x,y)\in\chi} (y-E(Y))^2p_{X,Y}(x,y)\right)^{\frac 12}\\
		&=\sqrt{\Var(X)\Var(Y)}
	\end{align*}

	So we have that $|\Cov(X,Y)|\leq \sqrt{\Var(X)\Var(Y)}$. That is,
	$$\left|\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}\right|=|\Corr(X,Y)|\leq 1$$

	So $-1\leq\Corr(X,Y)\leq 1$, as required.
\end{proof}

\begin{example}[]
	Consider the set up of Example \ref{ex:dice_jointpmf}. Find $\Cov(X,W)$ and $\Corr(X,W)$.
	\textit{Solution.} Recall that $\Cov(X,W)=E(XW)-E(X)E(W)$.
	Recall that the marginal pmf of $W$ from Example \ref{ex:dice_marginalpmf} is $$p_W(w)=\begin{cases}
		\frac{2w-1}{36} & \text{if $w=1,\hdots, 6$}\\
		0 & \text{otherwise}
	\end{cases}.$$

	We also have that the marginal pmf of $X$ is $$p_X(x)=\begin{cases}
		\frac 16 & \text{if $x=1,\hdots, 6$}\\
		0 & \text{otherwise}\end{cases}.$$

	We compute $E(X)$ and $E(W)$.

	\begin{align*}
		E(X)&=\sum_xxp_X(x)\\
		&=\sum_{x=1}^6x\frac 16\\
		&=\frac 16+\frac 26+\cdots+\frac 66\\
		&=\frac{21}6\\
		&=\frac 72
	\end{align*}

	\begin{align*}
		E(W)&=\sum_wwp_W(w)\\
		&=\sum_{w=1}^6w\frac{2w-1}{36}\\
		&=(1)\frac{1}{36}+(2)\frac{3}{36}+\cdots+(6)\frac{11}{36}\\
		&=\frac{161}{36}
	\end{align*}

	We find the product moment of $X$ and $W$ using the joint pmf $p_{X,W}$ defined in Example \ref{ex:dice_jointpmf}.
	\begin{align*}
		E(XW)&=\sum_{(x,w)}xwp_{X,W}(x,w)\\
		&=\sum_{x=1}^6x\sum_{w=1}^6wp_{X,W}(x,w)\\
		&=\frac{154}{9}
	\end{align*}

	We now see the following.
	\begin{align*}
		\Cov(X,W)&=E(XW)-E(X)E(W)\\
		&=\frac{154}{9}-\frac 72\cdot\frac{161}{36}\\
		&=\frac{105}{72}
	\end{align*}

	Recall that $\Corr(X,W)=\frac{\Cov(X,W)}{\sqrt{\Var(X)\Var(W)}}$.

	We compute $\Var(X)$ and $\Var(W)$.

	\begin{align*}
		\Var(X)&=E(X^2)-E(X)^2\\
		&=\sum_xx^2p_X(x)-E(X)^2\\
		&=\sum_{x=1}^6\frac{x^2}{6}-E(X)^2\\
		&=\frac{1^2}{6}+\frac{2^2}{6}+\cdots+\frac{6^2}{6}-\left(\frac 72\right)^2\\
		&=\frac{105}{36}\\
		&=\frac{35}{12}
	\end{align*}

	\begin{align*}
		\Var(W)&=E(W^2)-E(W)^2\\
		&=\sum_ww^2p_W(w)-E(W)^2\\
		&=\sum_{w=1}^6w^2\frac{2w-1}{36}-E(V)^2\\
		&=1^2\cdot\frac{1}{36}+2^2\cdot\frac{3}{36}+\cdots+6^2\cdot\frac{11}{36}-\left(\frac{161}{36}\right)^2\\
		&=\frac{2555}{1296}
	\end{align*}

	Then we have the following.

	\begin{align*}
		\Corr(X,W)&=\frac{\Cov(X,W)}{\sqrt{\Var(X)\Var(W)}}\\
		&=\frac{\frac{105}{72}}{\sqrt{\frac{35}{12}\cdot\frac{2555}{1296}}}\\
		&\approx 0.6082
	\end{align*}
\end{example}

\begin{example}[]
	Consider the setup of Example \ref{ex:jointpdf_cxy}. Find $\Cov(X,Y)$ and $\Corr(X,Y)$.

	\textit{Solution.} Recall that $E(XY)=1$ from Example \ref{ex:jointpdf_E(XY)}. We compute $E(X)$ and $E(Y)$ using the marginal pmfs found in Exercise \ref{ex:jointpdf_cxy}.

	\begin{align*}
		E(X)&=\int_0^2x\cdot\frac x2\dd x\\
		&=\int_0^2\frac{x^2}{2}\dd x\\
		&=\left[\frac{x^3}6\right]_0^2\\
		&=\frac{8}{6}\\
		&=\frac{4}{3}
	\end{align*}

	\begin{align*}
		E(Y)&=\int_0^1y\cdot3y^2\dd y\\
		&=3\int_0^1y^3\dd y\\
		&=3\left[\frac{y^4}{4}\right]\\
		&=\frac{3}{4}
	\end{align*}

	So w see the following.
	\begin{align*}
		\Cov(X,Y)&=E(XY)-E(X)E(Y)\\
		&=1-\frac{4}{3}\cdot\frac{3}{4}\\
		&=0
	\end{align*}
\end{example}

\subsection{Conditional distributions}

\begin{definition}[conditional pmf]
	Let $X_1$ and $X_2$ be discrete random variables with joint pmf $p_{X_1,X_2}$. Let $p_{X_1}$ and $p_{X_2}$ denote the marginal pmfs of $X_1$ and $X_2$, respectively. Suppose $x_1$ is such that $p_{X_1}(x_1)>0$. The \textbf{conditional pmf} of $X_2$ given that $X_1=x_1$ is defined by
	$$p_{X_2|X_1}(x_2|x_1)=\frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_1}(x_1)}$$
	where $x_2$ is in the support of $X_2$.
\end{definition}

As such, if $p_{X_2}(x_2)>0$, the conditional pmf of $X_1$ given that $X_2=x_2$ is
$$p_{X_1|X_2}(x_1|x_2)=\frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_2}(x_2)}$$
	where $x_1$ is in the support of $X_1$.

\begin{example}[]
	Recall Example \ref{ex:dice_jointpmf}. Find $E(W|X=3)$.

	\textit{Solution.} We require the conditional pmf of $W$ given that $X=3$. We have that $p_X(3)=6$. We see the following.
	\begin{align*}
		p_{W|X}(w|x=3)&=\frac{p_{W,X}(w,x)}{p_X(3)}
	\end{align*}

	\begin{center}
	\begin{tabular}{c | c}
		$w$ & $p_{W|X}(w|x=3)$\\ \hline
		1 & $0$\\
		2 & $0$\\
		3 & $\frac{1}{2}$\\
		4 & $\frac{1}{6}$\\
		5 & $\frac{1}{6}$\\
		6 & $\frac{1}{6}$
	\end{tabular}
	\end{center}

	\begin{align*}
		E(W|X=3)&=\sum_wwp_{W|X}(w|x=3)\\
		&=3\cdot\frac{1}{2}+4\cdot\frac{1}{6}+5\cdot\frac{1}{6}+6\cdot\frac{1}{6}\\
		&=\frac{24}{6}\\
		&=4
	\end{align*}
\end{example}

\begin{example}[]
	Suppose an urn contains 9 coloured balls: {\color{red}1 red}, {\color{green}3 green} and {\color{blue}5 blue}. Two such balls are drawn at random without replacement. Let {\color{red}$X$} denote the number of red balls drawn and {\color{green}$Y$} the number of green balls drawn. Find the conditional pmf of $X$ given that $Y=1$.

	\textit{Solution.} We first need the joint distribution of $(X,Y)$.

	By principles of counting, $p_{X,Y}(x,y)=\frac{\ncr{3}{y}}{\ncr{9}{2}}$

	Notice that the 
	\begin{center}
	\begin{tabular}{c | c c c}
		$p_{X,Y}(x,y)$ & $y=0$ & $y=1$ & $y=2$\\ \hline
		{\color{red}$x=0$} & $\frac{10}{36}$ & $\frac{15}{36}$\\
		{\color{red}$x=1$} & $\frac 5{36}$ & $\frac 3{36}$ & 0
	\end{tabular}
	\end{center}

	We are conditioning on $Y$. We must sum the highlighted column to obtain $p_Y(1)$.
	\begin{center}
	\begin{tabular}{c | c c c}
		$p_{X,Y}(x,y)$ & $y=0$ & $y=1$ & $y=2$\\ \hline
		$x=0$ & $\frac{10}{36}$ & $\frac{15}{36}$\\
		$x=1$ & $\frac 5{36}$ & $\frac 3{36}$ & 0
	\end{tabular}
	\end{center}

	We find our conditional probability to be as follows.

\end{example}



\begin{definition}[conditional pdf]
	Let $X_1$ and $X_2$ be continuous random variables with joint pdf $f_{X_1,X_2}$. Let $f_{X_1}$ and $f_{X_2}$ denote the marginal pdfs of $X_1$ and $X_2$, respectively. Suppose $x_1$ is such that $f_{X_1}(x_1)>0$. The \textbf{conditional pdf} of $X_2$ given that $X_1=x_1$ is defined by
	$$f_{X_2|X_1}(x_2|x_1)=\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_1}(x_1)}$$
	where $x_2$ is in the support of $X_2$.
\end{definition}

\begin{example}[]
	Recall Example \ref{ex:jointpdf_cxy}. Find $f_{Y|X}(y|x)$.
{}
	\textit{Solution.} First recall the joint pdf.
	$$f_{X,Y}(x,y)=\begin{cases}
		\frac 32 xy^2 & \text{if }0\leq x\leq 2,0\leq y\leq 1\\
		0 & \text{otherwise}
	\end{cases}$$

	Recall from Example [idk] the marginal pdf of $X$.
	$$f_X(x)=\begin{cases}
		\frac 12 x & \text{if }0\leq x\leq 2\\
		0 & \text{otherwise}
	\end{cases}$$

	We find the conditional pdf as follows.
	\begin{align*}
		f_{Y|X}(y|x)&=\frac{f_{X,Y}(x,y)}{f_{X}(x)}\\
		&=\frac{\frac 32 xy^2}{\frac 12 x}\\
		&=3y^3
	\end{align*}

	We find that $$f_{Y|X}(y|x)=\begin{cases}
		3y^3 & \text{if }0\leq y\leq 1\\
		0 & \text{otherwise}
	\end{cases}$$
\end{example}

\begin{example}[]
	Suppose $(X,Y)$ is continuous with joint pdf
	$$f_{X,Y}(x,y)=\begin{cases}
		8 xy & \text{if }0\leq x\leq y\leq 1\\
		0 & \text{otherwise}
	\end{cases}.$$
	Find the conditional pdf of $Y$ given $X=x$.

	\textit{Solution.} We first find the marginal pdf of $X$.

	\begin{align*}
		f_X(x)&=\int_yf_{X,Y}(x,y)\dd y\\
		&=\int_x^18xy\dd y\\
		&=4x(1-x^2)
	\end{align*}
\end{example}

\begin{theorem}[Rao-Blackwell Principle]
	Suppose that $(X_1,X_2)$ is a random vector such that $\Var(X_2)$ is finite. Then the following hold.
	\begin{enumerate}
		\item $E(E(X_2|X_1))=E(X_2)$
		\item $\Var(E(X_2|X_1))\leq\Var(X_2)$
	\end{enumerate}
\end{theorem}

\begin{proof}
	We prove the continuous case, beginning with the first statement.

	\begin{align*}
		E(E(X_2|X_1))&=\int_{-\infty}^{\infty}E(X_2|X_1=x_1)f_{X_1}(x_1)\dd{x_1}\\
		&=\int_{-\infty}^{\infty}\left(\int_{-\infty}^\infty x_2\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_1}(x_1)}\dd{x_2}\right)f_{X_1}(x_1)\dd{x_1}\\
		&=\int_{-\infty}^{\infty}\int_{-\infty}^\infty x_2f_{X_1,X_2}(x_1,x_2)\dd{x_2}\dd{x_1}\\
		&=E(X_2)
	\end{align*}

	The second statement follows from the law of total variance????
\end{proof}

\begin{definition}[conditional expectation]
	Let $(X,Y)$ be a random vector.

	Suppose that $(X,Y)$ is discrete. Then the \textbf{conditional expectation} of $Y$ given that $X=x$ is given by
	$$E(Y|X=x)=\sum_yyp_{Y|X}(y|x).$$

	Suppose that $(X,Y)$ is continuous. Then the \textbf{conditional expectation} of $Y$ given that $X=x$ is given by
	$$E(Y|X=x)=\int_yyf_{Y|X}(y|x)\dd y.$$

	$$E(u(Y)|X=x)=\int_yu(y)f_{Y|X}(y|x)\dd y$$
\end{definition}

It is not evident that $E(X_2|X_1)$ is a random variable. Consider a real-valued function $h$ and consider a random variable $X$. We see that $h(X)$ is a random variable and it takes on the value $h(x)$ if $X=x$. Recall that $E(Y|X=x)=\sum_yyp_{Y|X}(y|x)$. This simply gives a number depending on $x$. That is, it is a \textit{function} of $x$, say $g(x)=(Y|X=x)$. Similarly to $h(X)$, we can say that $g(X)$ is a random variable. In particular it is a random variable that takes on the value $g(x)=E(Y|X=x)$ if $X=x$.

\subsection{Independent Random Variables}

\begin{definition}[independence]
	Let $X_1$ and $X_2$ be a discrete random variables with joint pmf $p_{X_1,X_2}$ and marginal pmfs $p_{X_1}$ and $p_{X_2}$, respectively. The random variables $X_1$ and $X_2$ are said to be \textbf{independent} if $p_{X_1,X_2}(x_1,x_2)\equiv p_{X_1}(x_1)p_{X_2}(x_2)$.

	Similarly, let $X_1$ and $X_2$ be a continuous random variables with joint pdf $f_{X_1,X_2}$ and marginal pdfs $f_{X_1}$ and $f_{X_2}$, respectively. The random variables $X_1$ and $X_2$ are said to be \textbf{independent} if $f_{X_1,X_2}(x_1,x_2)\equiv f_{X_1}(x_1)f_{X_2}(x_2)$.

	Random variables that are not independent are said to be \textbf{dependent}.
\end{definition}

We may also define independence in terms of conditional pmfs and pdfs. That is, two random variables are independent if and only if
$$p_{X_2|X_1}(x_2|x_1)=p_{X_2}(x_2).$$

\begin{theorem}[]
	If $X$ and $Y$ are independent random variables, then $\Corr(X,Y)=0$.
\end{theorem}

\begin{proof}
	We prove the continuous case.

	Consider the product moment $E(XY)$.

	\begin{align*}
		E(XY)&=\int_y\int_xxyf_{X,Y}(x,y)\dd x\dd y\\
		&=\int_y\int_xxyf_{X}(x)f_Y(y)\dd x\dd y\\
		&\int_xxf_X(x)\dd x\int_yyf_Y(y)\dd y\\
		&=E(X)E(Y)
	\end{align*}

	Now we see that $\Cov(X,Y)=E(XY)-E(X)E(Y)=0$. As such, we also have that $\Corr(X,Y)=0$, as required.
\end{proof}

If $\Corr(X,Y)=0$, it need not be the case that $X$ and $Y$ are independent. We give a counterexample.

\begin{theorem}[]
	Suppose that $X_1$ and $X_2$ have supports $\mathcal S_1$ and $\mathcal S_2$, respectively, and their joint pdf is $f_{X_1,X_2}$. Then $X_1$ and $X_2$ are independent if and only if there exist nonnegative real-valued functions $g$ and $h$ such that $$f_{X_1,X_2}(x_1,x_2)=g(x_1)h(x_2)$$ for all $(x_1,x_2)\in\mathbb R^2$ where $g(x_1)>0$ for all $x_1\in\mathcal S_1$ and $h(x_2)>0$ for all $x_2\in\mathcal S_2$.
\end{theorem}

\begin{definition}[joint mgf]
	Let $(X_1,X_2)$ be a random vector and suppose that for some $h_1,h_2>0$, the expectation $E(e^{t_1X_1+t_2X_2})$ exists for all $|t_1|<h_2$ and $|t_2|<h_2$. Then the \textbf{joint mgf} of $(X_1,X_2)$ is given by
	$$M_{X_1,X_2}(t_1,t_2)=E(e^{t_1X_1+t_2X_2})$$
	for all $|t_1|<h_2$ and $|t_2|<h_2$.
\end{definition}

\begin{theorem}[]
	Suppose the joint mgf $M_{X_1,X_2}(t_1,t_2)$ exists for the random variables $X_1$ and $X_2$. Then $X_1$ and $X_2$ are independent if and only if $$M_{X_1,X_2}(t_1,t_2)=M_{X_1,X_2}(t_1,0)M_{X_1,X_2}(0,t_2).$$
\end{theorem}

Note that $M_{X_1,X_2}(t_1,0)=M_{X_1}(t_1)$ and $M_{X_1,X_2}(0,t_2)=M_{X_2}(t_2)$. That is, the product in the above theorem is the product of the \textit{marginal} mgfs of $X_1$ and $X_2$.

\subsection{Bivariate transformations}

Let $(X_1,X_2)$ be a random vector. We wish to find the distribution of $g(X_1,X_2)$. Observe that if $\chi$ is the support of $(X_1,X_2)$, then the support of $Y=g(X_1,X_2)$ is $\mathcal Y=\{y=g(x_1,x_2):(x_1,x_2)\in\chi\}$.

When $(X_1,X_2)$ is discrete with joint pmf $p_{X_1,X_2}$, the pmf of $Y$ can simply be written as

\begin{align*}
	p_Y(y)&=P(g(X_1,X_2)=y)\\
	&=\sum_{(x_1,x_2)\in A_y}p_{X_1,X_2}(x_1,x_2)
\end{align*}

Where $A_y=\{(x_1,x_2)\in\chi:y=g(x_1,x_2)\}$.

\begin{example}[]
	Let $X$ be a random variable with pmf
	$$p_X(x)=\begin{cases}
		\frac 13 &\text{if $x=0,3$}\\
		\frac 16 &\text{if $x=1,2$}\\
		0 &\text{otherwise}
	\end{cases}$$
	Find the distribution of $Y=X+2$.

	\textit{Solution.} Observe that the support of $Y$ is $\{2,3,4,5\}$. We simply have that $p_Y(y)=p_X(y-2)$.
\end{example}

We may also use a cdf to determine transformations. We first review the univariate case. If $X$ is a random variable, with pdf $f_X(x)$ in the continuous case or pmf $p_X(x)$ in the discrete case, it is often easier to find the distribution of $Y=g(X)$ by finding the cdf of $Y$.

For example, in the continuous case, 

\begin{example}
	Suppose $X$ is a continuous random variable with pdf $f_X(x)=\begin{cases}
		\frac 34 & \text{if $0\leq x\leq 1$}\\
		\frac 14 & \text{if $1\leq x\leq 2$}\\
		0 & \text{otherwise}
	\end{cases}$.
	Find the pdf of $Y=5X$.

	\textit{Solution.} First note that the support of $Y$ is the interval $[0,10]$. We see the following.
	\begin{align*}
		F_Y(y)&=P(Y\leq y)\\
		&=P(5X\leq y)\\
		&=P(X\leq 5y)\\
		&=\int_0^{y/5}f_X(x)\dd x\\
		&=\begin{cases}
			0 & \text{for $y<0$}\\
			\int_0^{y/5}\frac 34\dd x & \text{for $0\leq y \leq 5$}\\
			\int_0^{1}\frac 34\dd x+\int_1^{y/5}\frac 14\dd x & \text{for $5\leq y \leq 10$}\\
			1 & \text{for $y>10$}
		\end{cases}\\
		&=\begin{cases}
			0 & \text{for $y<0$}\\
			\frac 3{20}y & \text{for $0\leq y \leq 5$}\\
			\frac 34+\frac 14(y/5-1) & \text{for $5\leq y \leq 10$}\\
			1 & \text{for $y>10$}
		\end{cases}
	\end{align*}

	We can now find the pdf of $Y$ by differentiating.
\end{example}

\begin{example}[]
	Let $X$ be a continuous random variable with pdf $f_X(x)=\begin{cases}
		2e^{-2x} & \text{if $x>0$}\\
		0 & \text{otherwise}
	\end{cases}$.
	Let $Y=\sqrt Y$. Find the pdf of $Y$.

	\textit{Solution.} First note that the support of $Y$ is the interval $(0,\infty)$. Using the cdf approach we may find the cdf of $Y$.
	\begin{align*}
		F_Y(y)&=P(Y\leq y)\\
		&=P(\sqrt X\leq y)\\
		&=P(X\leq y^2)\\
		&=\begin{cases}
			0 & \text{if $y<0$}\\
			\int_0^{y^2}2e^{-2x} & \text{if $y^2\geq0$}
		\end{cases}\\
		&=\begin{cases}
			0 & \text{if $y<0$}\\
			1-e^{-2y^2} & \text{if $y^2\geq0$}
		\end{cases}
	\end{align*}

	We can now find the pdf of $Y$ by differentiating.
	$$f_Y(y)=\begin{cases}
			0 & \text{if $y<0$}\\
			4ye^{-2y^2} & \text{if $y\geq0$}
		\end{cases}$$
\end{example}

In the bivariate case, if $(X_1,X_2)$ is continuous with joint pdf $f_{X_1,X_2}$, then to find the distribution of $Y=g(X_1,X_2)$, like in the univariate case, it is often easier to find the cdf of $Y$.

\begin{example}[]
	Let $(X_1,X_2)$ denote a random sample of size $n=2$ from $U(0,1)$. Find the distribution of $Y=X_1+X_2$.

	\textit{Solution.} First note the pdf of $X_1$ and $X_2$.
	$$f_{X_1}(x_1)=\begin{cases}
		1 & \text{if $0\leq x\leq 1$}\\
		0 & \text{otherwise}
	\end{cases}$$
	The pdf of $X_2$ is identical. Since $X_1$ and $X_2$ are IID, their joint pdf is simply
	$$f_{X_1,X_2}(x_1,x_2)=\begin{cases}
		1 & \text{if $0\leq x_1\leq 1,0\leq x_2\leq 1$}\\
		0 & \text{otherwise}
	\end{cases}.$$
	We would like to find $F_Y(y)=P(Y\leq y)$. The joint pdf is illustrated below.

	Consider the region $0\leq x_1+x_2\leq y\leq 1$

	\begin{center}
	\begin{tikzpicture}
	\begin{axis}[
	GridPlot1,
	xmin=0,
	xmax=2.25,
	ymin=0,
	ymax=2.25,
	xtick={0,1,2},
	ytick={0,1,2},
	xlabel={$x_1$},
	ylabel={$x_2$},
    scale=0.75
	%axis equal
	]
	\path[name path=axis] (axis cs:0,0) -- (axis cs:1,0);
	\path[name path=y] (0,1) -- (1,0);

	%\node at (1.5,1.25) {\color{indigo}$\chi$};

	 \addplot[
        thick,
        color=main,
        fill opacity=0.2]
    fill between[
        of=y and axis,
        soft clip={domain=0:1},
    ];

    \addplot[thick,main] {1-x};

	\end{axis}
	\end{tikzpicture}
	\end{center}

	In the case that $1\leq y\leq 2$, we have a slightly more complicated region.

	\begin{center}
	\begin{tikzpicture}
	\begin{axis}[
	GridPlot1,
	xmin=0,
	xmax=2.25,
	ymin=0,
	ymax=2.25,
	xtick={0,1,2},
	ytick={0,1,2},
	xlabel={$x_1$},
	ylabel={$x_2$},
    scale=0.75
	%axis equal
	]
	\path[name path=axis] (axis cs:0,0) -- (axis cs:1,0);
	\path[name path=y] (0,1) -- (0.5,1) -- (1,0.5) -- (1,0);

	%\node at (1.5,1.25) {\color{indigo}$\chi$};

	 \addplot[
        thick,
        color=main,
        fill opacity=0.2]
    fill between[
        of=y and axis,
        soft clip={domain=0:1},
    ];

    \addplot[thick,main] {1.5-x};

	\end{axis}
	\end{tikzpicture}
	\end{center}

	Here we see the following.
	\end{example}

\subsubsection*{Jacobian method}

Let $X$ be a continuous random variable with pdf $f_X$ and support $\mathcal S_X$. Let $Y=g(X)$ where $g$ is injective and $g\in C^1(\mathcal S_X)$. Let $g^{-1}$ denote the inverse of $g$ and let $\frac{\dd x}{\dd y}=\frac{\dd{}}{\dd y}g^{-1}(y)$. Then then pdf of $Y$ is $$f_Y(y)=f_X(g^{-1}(y))\left|\frac{\dd x}{\dd y}\right|$$ for $y\in\mathcal S_Y$ where $\mathcal S_Y=\{y=g(x):x\in\mathcal S_X\}$.

\begin{example}[]\label{ex:exptheta}
	Let $X$ be an $\operatorname{Exp}(\theta)$ random variable with pdf
	$$f_X(x)=\begin{cases}
		\frac 1\theta e^{-\frac x\theta} & \text{if $x\geq 0$}\\
		0 & \text{otherwise}
	\end{cases}$$
	where $\theta >0$. Let $Y=5X$. Find the pdf of $Y$ using the Jacobian approach.

	\textit{Solution.} We compute.
	\begin{align*}
		f_Y(y)&=f_X(y/5)\left|\frac 15\right|\\
		&=\frac 1\theta e^{-\frac{y/5}\theta}\left|\frac 15\right|\\
		&=\frac 1{5\theta} e^{-\frac{y}{5\theta}}
	\end{align*}
	We obserce that $Y\sim\operatorname{Exp}(5\theta)$.
\end{example}

Let $(X_1,X_2)$ be a random vector with support $\chi$. We may seek the distribution of $Y=g(X_1,X_2)$. In this case, we can often find another random variable $Z=h(X_1,X_2)$ that is an injective function of $(X_1,X_2)$.

In this case, we can invert the transformation.
\begin{align*}
	X_1&=u_1(Y,Z)\\
	X_2&=u_2(Y,Z)\\
\end{align*}

So our bivariate Jacobian follows.

\begin{align*}
	J&=\begin{bmatrix}
	\frac{\partial x_1}{\partial y} & \frac{\partial x_1}{\partial z}\\
	\frac{\partial x_2}{\partial y} & \frac{\partial x_2}{\partial z}
	\end{bmatrix}\\
	&=\begin{bmatrix}
	\frac{\partial u_1(y,z)}{\partial y} & \frac{\partial u_1(y,z)}{\partial z}\\
	\frac{\partial u_2(y,z)}{\partial y} & \frac{\partial u_2(y,z)}{\partial z}
	\end{bmatrix}\\
\end{align*}

Using the Jacobian, we can find the joint pdf of $(Y,Z)$ as
$$f_{Y,Z}(y,z)=f_{X_1,X_2}(u_1(y,z),u_2(y,z))|J|$$
where $(y,z)\in\tau$ and $\tau=\{(y,z)\in\mathbb R^2:(u_1(y,z),u_2(y,z))\in\chi\}$.

Usually we are interested in only $Y$ so we may find
$$f_Y(y)=\int_zf_{Y,Z}(y,z)\dd z.$$

\begin{example}[]
	Let $X\sim\operatorname{Gamma}(\alpha_1,1)$ and, independently, $Y\sim\operatorname{Gamma}(\alpha_2,1)$. Since $X$ and $Y$ are independent, their joint pdf is
	$$f_{X,Y}=\begin{cases}
		\frac{1}{\Gamma(\alpha_1)}e^{-x}x^{\alpha_1-1}\frac{1}{\Gamma(\alpha_2)}e^{-y}y^{\alpha_2-1} & \text{if $x,y>0$}\\
		0 & \text{otherwise}
	\end{cases}.$$
	Find the distribution of $U=\frac{X}{X+Y}$.

	\textit{Solution.} Let $V=X+Y$ so that $U=\frac{X}{V}$. Inverting, we find that $X=UV$ and $Y=V(1-U)$. Our Jacobian is thus
	\begin{align*}
		J&=\begin{bmatrix}
			\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v}\\
			\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}\\
		\end{bmatrix}\\
		&=\begin{bmatrix}
			v & u\\
			-v & 1-u
		\end{bmatrix}
	\end{align*}
	We see that $\det(J)=v$. 
\end{example}

\subsubsection*{Moment generating function method}

Recall that mgfs uniquely determine the distribution of a random variable. As such, we may use mgfs to find distributions of transformed random variables.

\begin{example}[]
	Recall the setup of Example \ref{ex:exptheta} but use the mgf method.

	\textit{Solution.}
	\begin{align*}
		M_Y(t)&=E(e^{tY})\\
		&=E(e^{t\cdot 5X})\\
		&=E(e^{(5t)X})\\
		&=M_X(5t)\\
		&=(1-5\theta t)
	\end{align*}
	We clearly see that $Y\sim\operatorname{Exp}(5\theta)$.
\end{example}


%THIS ENDS MATERIAL COVERED IN LECTURE. THE FOLLOWING HAS BEEN TYPESET TENTATIVELY.
