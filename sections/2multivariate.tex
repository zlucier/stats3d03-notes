\rhead{\color{main}2 Multivariate distributions}
\section{Multivariate distributions}

We will often want to deal with more than one variable based on the same random experiment.

\begin{definition}[random vector]
	Consider a random experiment with sample space $\Omega$. Let $X_1,X_2:\Omega\to\mathbb R$ be RVs. We say that $(X_1,X_2)$ is a \textbf{random vector}. The support of $(X_1,X_2)$ is the set of ordered pairs $\chi=\{(x_1,x_2):X_1(\omega)=x_1,X_2(\omega)=x_2,\omega\in \Omega\}$.
\end{definition}

Of particular interest are events of the forms $(X_1\leq x_1)\cap(X_2\leq x_2)$ for $(x_1,x_2)\in\mathbb R^2$

\begin{definition}[joint cdf]
	Suppose that $(X_1,X_2)$ is a random vector. The \textbf{joint cdf} is defined as $$F_{X_1,X_2}(x_1,x_2)=P((X_1\leq x_1)\cap(X_2\leq x_2)).$$
\end{definition}

From here we can extend the univariate case for the probability over intervals to rectangular subsets of $\mathbb R^2$.

\begin{theorem}[]
	Suppose that the random vector $(X_1,X_2)$ has joint cdf $F_{X_1,X_2}$ and let $a_1,b_1,a_2,b_2\in\mathbb R$ be such that $a_1<b_1$ and $a_2<b_2$. Then $$P((X_1,X_2)\in[a_1,b_1]\times[a_2,b_2])=F_{X_1,X_2}(b_1,b_2)-F_{X_1,X_2}(a_1,b_2)-F_{X_1,X_2}(b_1,a_2)+F_{X_1,X_2}(a_1,a_2).$$
\end{theorem}

Recall that, in general, RVs can be of the discrete type or of the continuous type. We extend this idea to random vectors.

A random vector $(X_1,X_2)$ is said to be \textbf{discrete} if its support $\chi$ is countable. In this case both $X_1$ and $X_2$ are discrete RVs. It thus makes sense to define pmfs for random vectors.

\begin{definition}[joint pmf]
	Let $(X_1,X_2)$ be a discrete random vector. Then the \textbf{joint pmf} of $(X_1,X_2)$ is given by

	$$p_{X_1,X_2}(x_1,x_2)=P(X_1=x_1,X_2=x_2).$$
\end{definition}

As in the univariate case, this joint pmf satisfies the following properties.
\begin{enumerate}
	\item $0\leq p_{X_1,X_2}(x_1,x_2)\leq 1$ for all $(x_1,x_2)\in\chi$.
	\item $\sum_{(x_1,x_2)\in\chi}p_{X_1,X_2}(x_1,x_2)=1$.
\end{enumerate}

\begin{example}[]
	Suppose two dice are rolled. Let $X$ denote the number of dots facing up on the first die and $Y$ the number of dots on the second die. Also, let $W$ denote the larger of the two. We would like to find the joint pmf of $(X,W)$.

	By the equally likely model, the joint pmf of $(X,W)$ is summarized by the following table.

	\begin{center}
	\begin{tabular}{c | c c c c c c}
		$W/X$ & 1 & 2 & 3 & 4 & 5 & 6\\\hline
		1 & $\frac{1}{36}$ & 0 & 0 & 0 & 0 & 0\\
		2 & $\frac{1}{36}$ & $\frac{2}{36}$ & 0 & 0 & 0 & 0\\
		3 & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{3}{36}$ & 0 & 0 & 0\\
		4 & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{4}{36}$ & 0 & 0\\
		5 & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{5}{36}$ & 0\\
		6 & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{1}{36}$ & $\frac{6}{36}$
	\end{tabular}
	\end{center}

	Note that it is clearly the case that $0\leq p_{X,W}(x,w)\leq 1$ and $\sum_{(x,w)\in(X,W)}p_{X,W}(x,w)=1$.

	Suppose we wanted to find 
	\begin{align*}
		\sum_{\substack{(x,w)\in\chi\\x=w}}p_{X,W}(x,w)\\
		&=\frac{7}{12}
	\end{align*}
\end{example}

If the joint cdf of a random vector $(X_1,X_2)$ is continuous, then we say that $(X_1,X_2)$ is continuous. Similarly to a joint pmf, we can also define a joint pdf

\begin{definition}[joint pdf]
	Let $(X_1,X_2)$ be a continuous random vector. Then the \textbf{joint pdf} of $(X_1,X_2)$ is the function $f_{X_1,X_2}:\mathbb R^2\to\mathbb R_+$ satisfying
	$$F_{X_1,X_2}(x_1,x_2)>\int_{-\infty}^{x_2}\int_{-\infty}^{x_1}f_{X_1,X_2}(u,v)\dd u\dd v.$$
\end{definition}

As in the univariate case, the pdf satisfies the following properties

\begin{enumerate}
	\item $f_{X_1,X_2}(x_1,x_2)\geq 0$ for all $(x_1,x_2)\in\mathbb R^2$.
	\item $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{X_1,X_2}(u,v)\dd u\dd v=1$.
\end{enumerate}

We may also define the marginal cdf of a random vector $(X_1,X_2)$.

\begin{align*}
	F_{X_1}&=P(X_1\leq x_1)\\
	&=P(X_1\leq x_1,-\infty<X_2<\infty)\\
	&=\lim_{x_2\to\infty}P(X_1\leq x_1,X_2\leq x_2)\\
	&=\lim_{x_2\to\infty} F_{X_1,X_2}(x_1,x_2)
\end{align*}

We may also find marginal pmfs and pdf.

If $(X_1,X_2)$ is a discrete random vector, then
\begin{align*}
	p_{X_1}(x_1)&=P(X_1=x_1)\\
	&=\sum_{x_2}p_{X_1,X_2}(x_1,x_2)
\end{align*}

If $(X_1,X_2)$ is a continuous random vector, then
\begin{align*}
	f_{X_1}(x_1)&=\int_{-\infty}^{\infty}f_{X_1,X_2}(x_1,x_2)\dd{x_2}\\
\end{align*}

\begin{example}[]
	We revisit the previous example.

	\begin{center}
	\begin{tabular}{c | c}
		$W$ & $p_W(w)$\\ \hline
		1 & $\frac{1}{36}$\\
		2 & $\frac{3}{36}$\\
		3 & $\frac{5}{36}$\\
		4 & $\frac{7}{36}$\\
		5 & $\frac{9}{36}$\\
		6 & $\frac{11}{36}$
	\end{tabular}
	\end{center}

	With such a marginal pmf, we may find marginal expectations directly. In this case, $E(W)=\sum_1^6wp_W(w)$.
\end{example}


\begin{example}[]
	Consider the joint pdf of $(X,Y)$ to be
	$$f_{X,Y}(x,y)=\begin{cases}
		cxy^2 & \text{for $0\leq x\leq 2,0\leq y\leq 1$}\\
		0 & \text{otherwise}
	\end{cases}$$
	We would like to find the value of $c$ such that $f_{X,Y}$ is a valid pdf. We would then like to find the marginal pdfs.

	For $f_{X,Y}$ to be valid, we require it to be non-negative and it must integrate to 1. The second property will be used to determine $c$.

	\begin{align*}
		\int_0^1\int_0^2 cxy^2\dd x\dd y & =1\\
		c\int_0^1y^2\int_0^2 x\dd x\dd y & =1\\
		c\left(\frac 13\right)(2)&=1\\
		\frac 23 c&=1\\
		c&=\frac 32
	\end{align*}

	Such a $c$ makes $f_{X,Y}$ a valid pdf.

	We first find the marginal pdf of $X$.

	\begin{align*}
		f_X(x)&=\int_0^1\frac 32xy^2\dd y\\
		&=\frac 32x\int_0^1y^2\dd y\\
		&=\frac x2
	\end{align*}

	So $f_X(x)=\begin{cases}
		\frac x 2 & \text{for $0\leq x\leq 2$}\\
		0 & \text{otherwise}
	\end{cases}$

	We leave the marginal pdf of $Y$ as an exercise.
\end{example}

Expectations in the multivariate case are easily extended from the univariate case to random vectors.

Suppose $(X_1,X_2)$ is a random vector and let $Y=g(X_1,X_2)$ for $g:\mathbb R^2\to\mathbb R$.

If $(X_1,X_2)$ is discrete with joint pmf $p_{X_1,X_2}$, then $E(Y)$ exists if [insert condition] and is defined as $$E(Y)=\sum_{x_1}\sum_{x_2} g(x_1,x_2)p_{X_1,X_2}(x_1,x_2)$$
